<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=sDU-RIIs3Wq_4pUcDwWu-yDgXUdrr8dlHlnKX9v_9kmH4VK7Dpzzvc124h4xZ5pB7Mlga2LpaMBRA3oTejcG0A);ul.lst-kix_9qrkmc12qay3-7{list-style-type:none}ul.lst-kix_9qrkmc12qay3-8{list-style-type:none}ul.lst-kix_9qrkmc12qay3-1{list-style-type:none}ul.lst-kix_9qrkmc12qay3-2{list-style-type:none}ul.lst-kix_w2nv1w5i9k8v-8{list-style-type:none}ul.lst-kix_9qrkmc12qay3-0{list-style-type:none}ul.lst-kix_9qrkmc12qay3-5{list-style-type:none}ul.lst-kix_w2nv1w5i9k8v-5{list-style-type:none}ul.lst-kix_9qrkmc12qay3-6{list-style-type:none}ul.lst-kix_w2nv1w5i9k8v-4{list-style-type:none}ul.lst-kix_9qrkmc12qay3-3{list-style-type:none}ul.lst-kix_w2nv1w5i9k8v-7{list-style-type:none}ul.lst-kix_9qrkmc12qay3-4{list-style-type:none}.lst-kix_jj6p90k4m6me-5>li{counter-increment:lst-ctn-kix_jj6p90k4m6me-5}ul.lst-kix_w2nv1w5i9k8v-6{list-style-type:none}ol.lst-kix_jj6p90k4m6me-3.start{counter-reset:lst-ctn-kix_jj6p90k4m6me-3 0}ul.lst-kix_w2nv1w5i9k8v-1{list-style-type:none}ul.lst-kix_w2nv1w5i9k8v-0{list-style-type:none}ul.lst-kix_w2nv1w5i9k8v-3{list-style-type:none}ul.lst-kix_w2nv1w5i9k8v-2{list-style-type:none}.lst-kix_qebx5z30856i-0>li:before{content:"\002605   "}.lst-kix_nhlbzic3rjpp-8>li:before{content:"\0025a0   "}.lst-kix_qebx5z30856i-4>li:before{content:"\0025cb   "}.lst-kix_qebx5z30856i-5>li:before{content:"\0025a0   "}.lst-kix_nhlbzic3rjpp-6>li:before{content:"\0025cf   "}.lst-kix_nhlbzic3rjpp-7>li:before{content:"\0025cb   "}ul.lst-kix_b3xcjmtuwb81-2{list-style-type:none}.lst-kix_nhlbzic3rjpp-4>li:before{content:"\0025cb   "}.lst-kix_nhlbzic3rjpp-5>li:before{content:"\0025a0   "}ul.lst-kix_b3xcjmtuwb81-1{list-style-type:none}.lst-kix_m0qsfktae37s-6>li:before{content:"\0025cf   "}.lst-kix_m0qsfktae37s-7>li:before{content:"\0025cb   "}ul.lst-kix_b3xcjmtuwb81-4{list-style-type:none}ul.lst-kix_b3xcjmtuwb81-3{list-style-type:none}.lst-kix_m0qsfktae37s-4>li:before{content:"\0025cb   "}.lst-kix_m0qsfktae37s-5>li:before{content:"\0025a0   "}.lst-kix_m0qsfktae37s-8>li:before{content:"\0025a0   "}ul.lst-kix_b3xcjmtuwb81-0{list-style-type:none}.lst-kix_qebx5z30856i-3>li:before{content:"\0025cf   "}.lst-kix_nhlbzic3rjpp-1>li:before{content:"\0025cb   "}.lst-kix_qebx5z30856i-2>li:before{content:"\0025a0   "}ul.lst-kix_b3xcjmtuwb81-6{list-style-type:none}.lst-kix_qebx5z30856i-1>li:before{content:"\0025cb   "}.lst-kix_nhlbzic3rjpp-2>li:before{content:"\0025a0   "}.lst-kix_nhlbzic3rjpp-3>li:before{content:"\0025cf   "}ul.lst-kix_b3xcjmtuwb81-5{list-style-type:none}ul.lst-kix_b3xcjmtuwb81-8{list-style-type:none}ul.lst-kix_b3xcjmtuwb81-7{list-style-type:none}.lst-kix_s719nfpkgkn3-1>li:before{content:"\0025c6   "}.lst-kix_m0qsfktae37s-0>li:before{content:"\0027a2   "}.lst-kix_m0qsfktae37s-1>li:before{content:"\0025cb   "}.lst-kix_s719nfpkgkn3-2>li:before{content:"\0025cf   "}.lst-kix_m0qsfktae37s-2>li:before{content:"\0025a0   "}.lst-kix_m0qsfktae37s-3>li:before{content:"\0025cf   "}ol.lst-kix_jj6p90k4m6me-8.start{counter-reset:lst-ctn-kix_jj6p90k4m6me-8 0}.lst-kix_s719nfpkgkn3-0>li:before{content:"\002794   "}.lst-kix_ej4nnpt3di2x-4>li:before{content:"\0025c6   "}.lst-kix_s719nfpkgkn3-8>li:before{content:"\0025cf   "}.lst-kix_qebx5z30856i-7>li:before{content:"\0025cb   "}.lst-kix_ej4nnpt3di2x-3>li:before{content:"\0025cf   "}.lst-kix_qebx5z30856i-6>li:before{content:"\0025cf   "}.lst-kix_qebx5z30856i-8>li:before{content:"\0025a0   "}.lst-kix_ej4nnpt3di2x-2>li:before{content:"\0025a0   "}.lst-kix_ej4nnpt3di2x-0>li:before{content:"\002756   "}.lst-kix_ej4nnpt3di2x-1>li:before{content:"\0027a2   "}ul.lst-kix_p852979r53y3-0{list-style-type:none}ul.lst-kix_m0qsfktae37s-8{list-style-type:none}.lst-kix_p852979r53y3-1>li:before{content:"\0025cf   "}ul.lst-kix_p852979r53y3-3{list-style-type:none}ul.lst-kix_p852979r53y3-4{list-style-type:none}ul.lst-kix_p852979r53y3-1{list-style-type:none}.lst-kix_p852979r53y3-0>li:before{content:"\0025cf   "}ul.lst-kix_p852979r53y3-2{list-style-type:none}ul.lst-kix_m0qsfktae37s-3{list-style-type:none}ul.lst-kix_p852979r53y3-7{list-style-type:none}ul.lst-kix_m0qsfktae37s-2{list-style-type:none}ul.lst-kix_p852979r53y3-8{list-style-type:none}ul.lst-kix_m0qsfktae37s-1{list-style-type:none}.lst-kix_p852979r53y3-3>li:before{content:"\0025cf   "}ul.lst-kix_p852979r53y3-5{list-style-type:none}ul.lst-kix_m0qsfktae37s-0{list-style-type:none}ul.lst-kix_p852979r53y3-6{list-style-type:none}ul.lst-kix_m0qsfktae37s-7{list-style-type:none}ul.lst-kix_m0qsfktae37s-6{list-style-type:none}ul.lst-kix_m0qsfktae37s-5{list-style-type:none}ul.lst-kix_m0qsfktae37s-4{list-style-type:none}.lst-kix_p852979r53y3-2>li:before{content:"\0025cf   "}.lst-kix_e78lefxl84ok-1>li:before{content:"\0025cb   "}.lst-kix_p852979r53y3-8>li:before{content:"\0025cf   "}.lst-kix_s719nfpkgkn3-7>li:before{content:"\0025c6   "}.lst-kix_e78lefxl84ok-0>li:before{content:"  "}.lst-kix_e78lefxl84ok-2>li:before{content:"\0025a0   "}.lst-kix_p852979r53y3-7>li:before{content:"\0025cf   "}.lst-kix_ej4nnpt3di2x-5>li:before{content:"\0027a2   "}.lst-kix_s719nfpkgkn3-6>li:before{content:"\0025cb   "}.lst-kix_ej4nnpt3di2x-6>li:before{content:"\0025a0   "}.lst-kix_p852979r53y3-4>li:before{content:"\0025cf   "}.lst-kix_e78lefxl84ok-4>li:before{content:"\0025cb   "}.lst-kix_p852979r53y3-5>li:before{content:"\0025cf   "}.lst-kix_ej4nnpt3di2x-7>li:before{content:"\0025cf   "}.lst-kix_s719nfpkgkn3-3>li:before{content:"\0025cb   "}.lst-kix_s719nfpkgkn3-5>li:before{content:"\0025cf   "}.lst-kix_e78lefxl84ok-3>li:before{content:"\0025cf   "}.lst-kix_p852979r53y3-6>li:before{content:"\0025cf   "}.lst-kix_jj6p90k4m6me-1>li{counter-increment:lst-ctn-kix_jj6p90k4m6me-1}.lst-kix_ej4nnpt3di2x-8>li:before{content:"\0025c6   "}.lst-kix_s719nfpkgkn3-4>li:before{content:"\0025c6   "}.lst-kix_phb4rwukmkow-7>li:before{content:"\0025cb   "}.lst-kix_datxsq12qj0c-8>li:before{content:"\0025a0   "}.lst-kix_3arvx8vp4edq-5>li:before{content:"\0025a0   "}.lst-kix_e78lefxl84ok-5>li:before{content:"\0025a0   "}.lst-kix_phb4rwukmkow-3>li:before{content:"\0025cf   "}.lst-kix_3arvx8vp4edq-3>li:before{content:"\0025cf   "}.lst-kix_e78lefxl84ok-7>li:before{content:"\0025cb   "}.lst-kix_datxsq12qj0c-2>li:before{content:"\0025a0   "}.lst-kix_3arvx8vp4edq-1>li:before{content:"\0025cb   "}.lst-kix_phb4rwukmkow-1>li:before{content:"\0025cb   "}.lst-kix_datxsq12qj0c-0>li:before{content:"\002605   "}.lst-kix_vxf40eumpq0q-0>li:before{content:"\0025cf   "}.lst-kix_vxf40eumpq0q-2>li:before{content:"\0025a0   "}.lst-kix_vxf40eumpq0q-4>li:before{content:"\0025cb   "}.lst-kix_42om2jlkt9i7-8>li:before{content:"\0025a0   "}.lst-kix_42om2jlkt9i7-6>li:before{content:"\0025cf   "}.lst-kix_phb4rwukmkow-5>li:before{content:"\0025a0   "}.lst-kix_b3xcjmtuwb81-8>li:before{content:"\0025a0   "}ul.lst-kix_3zd8jpb99zva-7{list-style-type:none}.lst-kix_3zd8jpb99zva-8>li:before{content:"\0025a0   "}ul.lst-kix_3zd8jpb99zva-8{list-style-type:none}.lst-kix_9xd58wc8mdpn-1>li:before{content:"\0025cb   "}ul.lst-kix_3zd8jpb99zva-5{list-style-type:none}ul.lst-kix_3zd8jpb99zva-6{list-style-type:none}ul.lst-kix_3zd8jpb99zva-3{list-style-type:none}.lst-kix_3zd8jpb99zva-6>li:before{content:"\0025cf   "}ul.lst-kix_3zd8jpb99zva-4{list-style-type:none}.lst-kix_nhlbzic3rjpp-0>li:before{content:"\0025cf   "}ul.lst-kix_3zd8jpb99zva-1{list-style-type:none}ul.lst-kix_3zd8jpb99zva-2{list-style-type:none}ul.lst-kix_3zd8jpb99zva-0{list-style-type:none}.lst-kix_3zd8jpb99zva-2>li:before{content:"\0025a0   "}.lst-kix_53s3nxw2a9uy-6>li:before{content:"\0025cf   "}.lst-kix_3zd8jpb99zva-0>li:before{content:"\0027a2   "}.lst-kix_3zd8jpb99zva-4>li:before{content:"\0025cb   "}.lst-kix_9xd58wc8mdpn-5>li:before{content:"\0025a0   "}.lst-kix_vxf40eumpq0q-6>li:before{content:"\0025cf   "}ul.lst-kix_ci1ll0e8g7yl-4{list-style-type:none}.lst-kix_9xd58wc8mdpn-3>li:before{content:"\0025cf   "}ul.lst-kix_ci1ll0e8g7yl-5{list-style-type:none}ul.lst-kix_ci1ll0e8g7yl-2{list-style-type:none}ul.lst-kix_ci1ll0e8g7yl-3{list-style-type:none}.lst-kix_53s3nxw2a9uy-8>li:before{content:"\0025cf   "}ul.lst-kix_ci1ll0e8g7yl-0{list-style-type:none}ul.lst-kix_ci1ll0e8g7yl-1{list-style-type:none}.lst-kix_vxf40eumpq0q-8>li:before{content:"\0025a0   "}ul.lst-kix_ci1ll0e8g7yl-8{list-style-type:none}ol.lst-kix_jj6p90k4m6me-5.start{counter-reset:lst-ctn-kix_jj6p90k4m6me-5 0}ul.lst-kix_ci1ll0e8g7yl-6{list-style-type:none}ul.lst-kix_ci1ll0e8g7yl-7{list-style-type:none}.lst-kix_53s3nxw2a9uy-2>li:before{content:"\0025cf   "}.lst-kix_53s3nxw2a9uy-4>li:before{content:"\0025cf   "}.lst-kix_datxsq12qj0c-4>li:before{content:"\0025cb   "}.lst-kix_3arvx8vp4edq-7>li:before{content:"\0025cb   "}.lst-kix_53s3nxw2a9uy-0>li:before{content:"\0025cf   "}.lst-kix_datxsq12qj0c-6>li:before{content:"\0025cf   "}.lst-kix_t24wwq61hl5b-3>li:before{content:"\0025cf   "}.lst-kix_t24wwq61hl5b-0>li:before{content:"  "}.lst-kix_t24wwq61hl5b-4>li:before{content:"\0025cb   "}ul.lst-kix_9xd58wc8mdpn-2{list-style-type:none}ul.lst-kix_9xd58wc8mdpn-3{list-style-type:none}ul.lst-kix_9xd58wc8mdpn-0{list-style-type:none}.lst-kix_zbes9rxb87oe-2>li:before{content:"\0025a0   "}ul.lst-kix_9xd58wc8mdpn-1{list-style-type:none}ul.lst-kix_9xd58wc8mdpn-6{list-style-type:none}ul.lst-kix_9xd58wc8mdpn-7{list-style-type:none}.lst-kix_4d87yzwg7veb-4>li:before{content:"\0025cb   "}ul.lst-kix_3arvx8vp4edq-8{list-style-type:none}ul.lst-kix_9xd58wc8mdpn-4{list-style-type:none}.lst-kix_pugfjjme5q-7>li:before{content:"\0025cb   "}ul.lst-kix_3arvx8vp4edq-7{list-style-type:none}ul.lst-kix_9xd58wc8mdpn-5{list-style-type:none}ul.lst-kix_3arvx8vp4edq-6{list-style-type:none}ul.lst-kix_42om2jlkt9i7-4{list-style-type:none}.lst-kix_zbes9rxb87oe-5>li:before{content:"\0025a0   "}ul.lst-kix_3arvx8vp4edq-5{list-style-type:none}ul.lst-kix_42om2jlkt9i7-3{list-style-type:none}ul.lst-kix_3arvx8vp4edq-4{list-style-type:none}ul.lst-kix_42om2jlkt9i7-6{list-style-type:none}.lst-kix_zbes9rxb87oe-6>li:before{content:"\0025cf   "}ul.lst-kix_3arvx8vp4edq-3{list-style-type:none}ul.lst-kix_42om2jlkt9i7-5{list-style-type:none}ul.lst-kix_3arvx8vp4edq-2{list-style-type:none}ul.lst-kix_42om2jlkt9i7-8{list-style-type:none}ul.lst-kix_3arvx8vp4edq-1{list-style-type:none}ul.lst-kix_42om2jlkt9i7-7{list-style-type:none}.lst-kix_4d87yzwg7veb-3>li:before{content:"\0025cf   "}ul.lst-kix_3arvx8vp4edq-0{list-style-type:none}.lst-kix_ood6n1vrh4up-0>li:before{content:"\002605   "}ul.lst-kix_42om2jlkt9i7-0{list-style-type:none}.lst-kix_4d87yzwg7veb-0>li:before{content:"  "}ul.lst-kix_42om2jlkt9i7-2{list-style-type:none}ul.lst-kix_42om2jlkt9i7-1{list-style-type:none}ol.lst-kix_jj6p90k4m6me-0.start{counter-reset:lst-ctn-kix_jj6p90k4m6me-0 0}.lst-kix_ood6n1vrh4up-3>li:before{content:"\0025cf   "}.lst-kix_ood6n1vrh4up-4>li:before{content:"\0025cb   "}.lst-kix_ood6n1vrh4up-8>li:before{content:"\0025a0   "}.lst-kix_ci1ll0e8g7yl-2>li:before{content:"\0025cf   "}ul.lst-kix_nhlbzic3rjpp-6{list-style-type:none}ul.lst-kix_nhlbzic3rjpp-7{list-style-type:none}.lst-kix_9xd58wc8mdpn-7>li:before{content:"\0025cb   "}ul.lst-kix_nhlbzic3rjpp-8{list-style-type:none}ul.lst-kix_9xd58wc8mdpn-8{list-style-type:none}ul.lst-kix_nhlbzic3rjpp-2{list-style-type:none}ul.lst-kix_nhlbzic3rjpp-3{list-style-type:none}.lst-kix_9xd58wc8mdpn-8>li:before{content:"\0025a0   "}.lst-kix_t24wwq61hl5b-8>li:before{content:"\0025a0   "}.lst-kix_4d87yzwg7veb-7>li:before{content:"\0025cb   "}ul.lst-kix_nhlbzic3rjpp-4{list-style-type:none}ul.lst-kix_nhlbzic3rjpp-5{list-style-type:none}.lst-kix_t24wwq61hl5b-7>li:before{content:"\0025cb   "}.lst-kix_zbes9rxb87oe-1>li:before{content:"\0025cb   "}ul.lst-kix_nhlbzic3rjpp-0{list-style-type:none}ul.lst-kix_nhlbzic3rjpp-1{list-style-type:none}.lst-kix_ood6n1vrh4up-7>li:before{content:"\0025cb   "}.lst-kix_ci1ll0e8g7yl-5>li:before{content:"\0025cf   "}.lst-kix_4d87yzwg7veb-8>li:before{content:"\0025a0   "}.lst-kix_ci1ll0e8g7yl-6>li:before{content:"\0025cf   "}.lst-kix_b3xcjmtuwb81-5>li:before{content:"\0025a0   "}.lst-kix_tp8dcj8chtv6-7>li:before{content:"\0025cb   "}.lst-kix_42om2jlkt9i7-3>li:before{content:"\0025cf   "}.lst-kix_42om2jlkt9i7-0>li:before{content:"\002605   "}.lst-kix_42om2jlkt9i7-4>li:before{content:"\0025cb   "}.lst-kix_qvl8ngt12rhu-8>li:before{content:"\0025a0   "}.lst-kix_b3xcjmtuwb81-2>li:before{content:"\0025a0   "}.lst-kix_b3xcjmtuwb81-6>li:before{content:"\0025cf   "}ol.lst-kix_jj6p90k4m6me-1.start{counter-reset:lst-ctn-kix_jj6p90k4m6me-1 0}.lst-kix_tp8dcj8chtv6-6>li:before{content:"\0025cf   "}ul.lst-kix_3ftqqc8atfsd-6{list-style-type:none}.lst-kix_tp8dcj8chtv6-3>li:before{content:"\0025cf   "}ul.lst-kix_3ftqqc8atfsd-5{list-style-type:none}ul.lst-kix_3ftqqc8atfsd-4{list-style-type:none}ul.lst-kix_3ftqqc8atfsd-3{list-style-type:none}ul.lst-kix_3ftqqc8atfsd-8{list-style-type:none}ul.lst-kix_3ftqqc8atfsd-7{list-style-type:none}.lst-kix_ci1ll0e8g7yl-1>li:before{content:"\0025cf   "}ul.lst-kix_3ftqqc8atfsd-2{list-style-type:none}ul.lst-kix_3ftqqc8atfsd-1{list-style-type:none}ul.lst-kix_53s3nxw2a9uy-0{list-style-type:none}ul.lst-kix_3ftqqc8atfsd-0{list-style-type:none}.lst-kix_tp8dcj8chtv6-2>li:before{content:"\0025a0   "}ul.lst-kix_53s3nxw2a9uy-1{list-style-type:none}.lst-kix_b3xcjmtuwb81-1>li:before{content:"\0025cb   "}ul.lst-kix_t24wwq61hl5b-2{list-style-type:none}ul.lst-kix_t24wwq61hl5b-3{list-style-type:none}ul.lst-kix_t24wwq61hl5b-4{list-style-type:none}ul.lst-kix_t24wwq61hl5b-5{list-style-type:none}ul.lst-kix_t24wwq61hl5b-6{list-style-type:none}ul.lst-kix_t24wwq61hl5b-7{list-style-type:none}ul.lst-kix_t24wwq61hl5b-8{list-style-type:none}ul.lst-kix_53s3nxw2a9uy-2{list-style-type:none}ul.lst-kix_53s3nxw2a9uy-3{list-style-type:none}ul.lst-kix_53s3nxw2a9uy-4{list-style-type:none}ul.lst-kix_53s3nxw2a9uy-5{list-style-type:none}ul.lst-kix_53s3nxw2a9uy-6{list-style-type:none}ul.lst-kix_53s3nxw2a9uy-7{list-style-type:none}ul.lst-kix_53s3nxw2a9uy-8{list-style-type:none}ul.lst-kix_t24wwq61hl5b-0{list-style-type:none}ul.lst-kix_t24wwq61hl5b-1{list-style-type:none}ul.lst-kix_esb2ihnubxs-8{list-style-type:none}.lst-kix_qvl8ngt12rhu-0>li:before{content:"\0027a2   "}.lst-kix_qvl8ngt12rhu-4>li:before{content:"\0025cb   "}.lst-kix_qvl8ngt12rhu-1>li:before{content:"\0025cb   "}.lst-kix_qvl8ngt12rhu-5>li:before{content:"\0025a0   "}ul.lst-kix_esb2ihnubxs-2{list-style-type:none}ul.lst-kix_esb2ihnubxs-3{list-style-type:none}ul.lst-kix_esb2ihnubxs-0{list-style-type:none}.lst-kix_jj6p90k4m6me-4>li{counter-increment:lst-ctn-kix_jj6p90k4m6me-4}ul.lst-kix_esb2ihnubxs-1{list-style-type:none}ul.lst-kix_esb2ihnubxs-6{list-style-type:none}ul.lst-kix_esb2ihnubxs-7{list-style-type:none}ul.lst-kix_esb2ihnubxs-4{list-style-type:none}ul.lst-kix_esb2ihnubxs-5{list-style-type:none}.lst-kix_v612ti81tkyd-8>li:before{content:"\0025a0   "}ul.lst-kix_ej4nnpt3di2x-4{list-style-type:none}ul.lst-kix_ej4nnpt3di2x-5{list-style-type:none}ul.lst-kix_ej4nnpt3di2x-2{list-style-type:none}.lst-kix_e78lefxl84ok-6>li:before{content:"\0025cf   "}ul.lst-kix_ej4nnpt3di2x-3{list-style-type:none}ul.lst-kix_ej4nnpt3di2x-0{list-style-type:none}ul.lst-kix_ej4nnpt3di2x-1{list-style-type:none}.lst-kix_phb4rwukmkow-2>li:before{content:"\0025a0   "}.lst-kix_esb2ihnubxs-2>li:before{content:"\0025a0   "}.lst-kix_datxsq12qj0c-1>li:before{content:"\0025cb   "}.lst-kix_3arvx8vp4edq-2>li:before{content:"\0025a0   "}ul.lst-kix_ej4nnpt3di2x-8{list-style-type:none}ul.lst-kix_ej4nnpt3di2x-6{list-style-type:none}ul.lst-kix_ej4nnpt3di2x-7{list-style-type:none}.lst-kix_6rbjdsr9r5fa-1>li:before{content:"\0025cb   "}.lst-kix_esb2ihnubxs-6>li:before{content:"\0025cf   "}.lst-kix_vxf40eumpq0q-1>li:before{content:"\0025cb   "}ul.lst-kix_zbes9rxb87oe-6{list-style-type:none}ul.lst-kix_zbes9rxb87oe-5{list-style-type:none}ul.lst-kix_zbes9rxb87oe-4{list-style-type:none}ul.lst-kix_zbes9rxb87oe-3{list-style-type:none}.lst-kix_vxf40eumpq0q-5>li:before{content:"\0025a0   "}.lst-kix_6rbjdsr9r5fa-5>li:before{content:"\0025a0   "}ul.lst-kix_zbes9rxb87oe-8{list-style-type:none}ul.lst-kix_zbes9rxb87oe-7{list-style-type:none}ul.lst-kix_zbes9rxb87oe-2{list-style-type:none}.lst-kix_42om2jlkt9i7-7>li:before{content:"\0025cb   "}ul.lst-kix_zbes9rxb87oe-1{list-style-type:none}ul.lst-kix_zbes9rxb87oe-0{list-style-type:none}.lst-kix_phb4rwukmkow-6>li:before{content:"\0025cf   "}.lst-kix_w2nv1w5i9k8v-4>li:before{content:"\0025cb   "}.lst-kix_jj6p90k4m6me-2>li{counter-increment:lst-ctn-kix_jj6p90k4m6me-2}.lst-kix_9xd58wc8mdpn-0>li:before{content:"\002605   "}.lst-kix_w2nv1w5i9k8v-0>li:before{content:"  "}.lst-kix_w2nv1w5i9k8v-8>li:before{content:"\0025a0   "}.lst-kix_3zd8jpb99zva-7>li:before{content:"\0025cb   "}.lst-kix_9xd58wc8mdpn-4>li:before{content:"\0025cb   "}.lst-kix_jsy652pf3qrm-2>li:before{content:"\0025a0   "}.lst-kix_jsy652pf3qrm-6>li:before{content:"\0025cf   "}.lst-kix_3zd8jpb99zva-3>li:before{content:"\0025cf   "}.lst-kix_53s3nxw2a9uy-7>li:before{content:"\0025cf   "}.lst-kix_53s3nxw2a9uy-3>li:before{content:"\0025cf   "}ul.lst-kix_v612ti81tkyd-4{list-style-type:none}.lst-kix_v612ti81tkyd-0>li:before{content:"\0027a2   "}ul.lst-kix_v612ti81tkyd-5{list-style-type:none}ul.lst-kix_v612ti81tkyd-6{list-style-type:none}ul.lst-kix_v612ti81tkyd-7{list-style-type:none}ul.lst-kix_v612ti81tkyd-0{list-style-type:none}ul.lst-kix_v612ti81tkyd-1{list-style-type:none}ul.lst-kix_v612ti81tkyd-2{list-style-type:none}ul.lst-kix_v612ti81tkyd-3{list-style-type:none}.lst-kix_v612ti81tkyd-4>li:before{content:"\0025cb   "}.lst-kix_datxsq12qj0c-5>li:before{content:"\0025a0   "}.lst-kix_3arvx8vp4edq-6>li:before{content:"\0025cf   "}.lst-kix_bwz1irxjtfi9-3>li:before{content:"\0025cf   "}ul.lst-kix_v612ti81tkyd-8{list-style-type:none}.lst-kix_pugfjjme5q-3>li:before{content:"\0025cf   "}.lst-kix_9qrkmc12qay3-0>li:before{content:"\0025cf   "}.lst-kix_bwz1irxjtfi9-1>li:before{content:"\0027a2   "}.lst-kix_9qrkmc12qay3-1>li:before{content:"\0025cb   "}.lst-kix_bwz1irxjtfi9-2>li:before{content:"\0025a0   "}.lst-kix_9qrkmc12qay3-4>li:before{content:"\0025cb   "}ul.lst-kix_bwz1irxjtfi9-8{list-style-type:none}.lst-kix_9qrkmc12qay3-2>li:before{content:"\0025a0   "}.lst-kix_bwz1irxjtfi9-7>li:before{content:"\0025cf   "}.lst-kix_9qrkmc12qay3-3>li:before{content:"\0025cf   "}.lst-kix_bwz1irxjtfi9-0>li:before{content:"\002756   "}.lst-kix_bwz1irxjtfi9-8>li:before{content:"\0025c6   "}.lst-kix_jj6p90k4m6me-4>li:before{content:"" counter(lst-ctn-kix_jj6p90k4m6me-4,lower-latin) ". "}.lst-kix_9qrkmc12qay3-8>li:before{content:"\0025a0   "}ul.lst-kix_bwz1irxjtfi9-1{list-style-type:none}ul.lst-kix_6rbjdsr9r5fa-8{list-style-type:none}ul.lst-kix_bwz1irxjtfi9-0{list-style-type:none}ul.lst-kix_6rbjdsr9r5fa-7{list-style-type:none}ul.lst-kix_bwz1irxjtfi9-3{list-style-type:none}ul.lst-kix_6rbjdsr9r5fa-6{list-style-type:none}.lst-kix_jj6p90k4m6me-5>li:before{content:"" counter(lst-ctn-kix_jj6p90k4m6me-5,lower-roman) ". "}ul.lst-kix_bwz1irxjtfi9-2{list-style-type:none}ul.lst-kix_6rbjdsr9r5fa-5{list-style-type:none}ul.lst-kix_bwz1irxjtfi9-5{list-style-type:none}ul.lst-kix_6rbjdsr9r5fa-4{list-style-type:none}ul.lst-kix_bwz1irxjtfi9-4{list-style-type:none}ul.lst-kix_6rbjdsr9r5fa-3{list-style-type:none}.lst-kix_9qrkmc12qay3-5>li:before{content:"\0025a0   "}ul.lst-kix_bwz1irxjtfi9-7{list-style-type:none}ul.lst-kix_6rbjdsr9r5fa-2{list-style-type:none}.lst-kix_jj6p90k4m6me-6>li:before{content:"" counter(lst-ctn-kix_jj6p90k4m6me-6,decimal) ". "}ul.lst-kix_bwz1irxjtfi9-6{list-style-type:none}ul.lst-kix_6rbjdsr9r5fa-1{list-style-type:none}ul.lst-kix_6rbjdsr9r5fa-0{list-style-type:none}.lst-kix_jj6p90k4m6me-7>li:before{content:"" counter(lst-ctn-kix_jj6p90k4m6me-7,lower-latin) ". "}.lst-kix_9qrkmc12qay3-6>li:before{content:"\0025cf   "}.lst-kix_9qrkmc12qay3-7>li:before{content:"\0025cb   "}.lst-kix_jj6p90k4m6me-8>li:before{content:"" counter(lst-ctn-kix_jj6p90k4m6me-8,lower-roman) ". "}.lst-kix_3ftqqc8atfsd-1>li:before{content:"\0025cb   "}.lst-kix_jj6p90k4m6me-3>li:before{content:"" counter(lst-ctn-kix_jj6p90k4m6me-3,decimal) ". "}.lst-kix_3ftqqc8atfsd-0>li:before{content:"\0025cf   "}.lst-kix_jj6p90k4m6me-2>li:before{content:"" counter(lst-ctn-kix_jj6p90k4m6me-2,lower-roman) ". "}.lst-kix_jj6p90k4m6me-1>li:before{content:"" counter(lst-ctn-kix_jj6p90k4m6me-1,lower-latin) ". "}.lst-kix_jj6p90k4m6me-0>li:before{content:"" counter(lst-ctn-kix_jj6p90k4m6me-0,decimal) ". "}.lst-kix_3ftqqc8atfsd-2>li:before{content:"\0025a0   "}.lst-kix_3ftqqc8atfsd-3>li:before{content:"\0025cf   "}.lst-kix_3ftqqc8atfsd-4>li:before{content:"\0025cb   "}.lst-kix_3ftqqc8atfsd-6>li:before{content:"\0025cf   "}.lst-kix_3ftqqc8atfsd-5>li:before{content:"\0025a0   "}.lst-kix_3ftqqc8atfsd-7>li:before{content:"\0025cb   "}.lst-kix_3xcyqiocqw8e-4>li:before{content:"\0025cb   "}.lst-kix_3xcyqiocqw8e-6>li:before{content:"\0025cf   "}.lst-kix_3ftqqc8atfsd-8>li:before{content:"\0025a0   "}.lst-kix_3xcyqiocqw8e-3>li:before{content:"\0025cf   "}.lst-kix_3xcyqiocqw8e-7>li:before{content:"\0025cb   "}.lst-kix_3xcyqiocqw8e-0>li:before{content:"\002605   "}.lst-kix_3xcyqiocqw8e-2>li:before{content:"\0025a0   "}.lst-kix_3xcyqiocqw8e-8>li:before{content:"\0025a0   "}.lst-kix_3xcyqiocqw8e-1>li:before{content:"\0025cb   "}ul.lst-kix_3xcyqiocqw8e-0{list-style-type:none}ul.lst-kix_3xcyqiocqw8e-2{list-style-type:none}ul.lst-kix_3xcyqiocqw8e-1{list-style-type:none}ul.lst-kix_3xcyqiocqw8e-4{list-style-type:none}ul.lst-kix_3xcyqiocqw8e-3{list-style-type:none}ul.lst-kix_3xcyqiocqw8e-6{list-style-type:none}.lst-kix_jj6p90k4m6me-7>li{counter-increment:lst-ctn-kix_jj6p90k4m6me-7}ul.lst-kix_3xcyqiocqw8e-5{list-style-type:none}ol.lst-kix_jj6p90k4m6me-2.start{counter-reset:lst-ctn-kix_jj6p90k4m6me-2 0}.lst-kix_3xcyqiocqw8e-5>li:before{content:"\0025a0   "}ul.lst-kix_vxf40eumpq0q-8{list-style-type:none}ul.lst-kix_vxf40eumpq0q-6{list-style-type:none}ul.lst-kix_vxf40eumpq0q-7{list-style-type:none}.lst-kix_esb2ihnubxs-1>li:before{content:"\0025cb   "}ul.lst-kix_vxf40eumpq0q-4{list-style-type:none}ul.lst-kix_vxf40eumpq0q-5{list-style-type:none}ul.lst-kix_vxf40eumpq0q-2{list-style-type:none}.lst-kix_v612ti81tkyd-5>li:before{content:"\0025a0   "}ul.lst-kix_vxf40eumpq0q-3{list-style-type:none}.lst-kix_esb2ihnubxs-3>li:before{content:"\0025cf   "}.lst-kix_v612ti81tkyd-7>li:before{content:"\0025cb   "}.lst-kix_jj6p90k4m6me-0>li{counter-increment:lst-ctn-kix_jj6p90k4m6me-0}.lst-kix_6rbjdsr9r5fa-4>li:before{content:"\0025cb   "}.lst-kix_esb2ihnubxs-7>li:before{content:"\0025cb   "}.lst-kix_6rbjdsr9r5fa-0>li:before{content:"\0027a2   "}.lst-kix_esb2ihnubxs-5>li:before{content:"\0025a0   "}.lst-kix_6rbjdsr9r5fa-2>li:before{content:"\0025a0   "}ol.lst-kix_jj6p90k4m6me-4{list-style-type:none}ol.lst-kix_jj6p90k4m6me-5{list-style-type:none}ol.lst-kix_jj6p90k4m6me-6{list-style-type:none}ol.lst-kix_jj6p90k4m6me-7{list-style-type:none}ol.lst-kix_jj6p90k4m6me-8{list-style-type:none}ol.lst-kix_jj6p90k4m6me-0{list-style-type:none}ol.lst-kix_jj6p90k4m6me-1{list-style-type:none}.lst-kix_6rbjdsr9r5fa-6>li:before{content:"\0025cf   "}ol.lst-kix_jj6p90k4m6me-2{list-style-type:none}ol.lst-kix_jj6p90k4m6me-3{list-style-type:none}ul.lst-kix_qebx5z30856i-6{list-style-type:none}.lst-kix_6rbjdsr9r5fa-8>li:before{content:"\0025a0   "}ul.lst-kix_qebx5z30856i-5{list-style-type:none}ul.lst-kix_qebx5z30856i-8{list-style-type:none}ol.lst-kix_jj6p90k4m6me-7.start{counter-reset:lst-ctn-kix_jj6p90k4m6me-7 0}ul.lst-kix_qebx5z30856i-7{list-style-type:none}.lst-kix_w2nv1w5i9k8v-3>li:before{content:"\0025cf   "}.lst-kix_w2nv1w5i9k8v-1>li:before{content:"\0025cb   "}.lst-kix_w2nv1w5i9k8v-5>li:before{content:"\0025a0   "}.lst-kix_w2nv1w5i9k8v-7>li:before{content:"\0025cb   "}ol.lst-kix_jj6p90k4m6me-4.start{counter-reset:lst-ctn-kix_jj6p90k4m6me-4 0}.lst-kix_jsy652pf3qrm-1>li:before{content:"\0025cb   "}.lst-kix_jsy652pf3qrm-3>li:before{content:"\0025cf   "}ul.lst-kix_ood6n1vrh4up-0{list-style-type:none}ul.lst-kix_ood6n1vrh4up-2{list-style-type:none}ul.lst-kix_ood6n1vrh4up-1{list-style-type:none}ul.lst-kix_ood6n1vrh4up-4{list-style-type:none}ul.lst-kix_ood6n1vrh4up-3{list-style-type:none}ul.lst-kix_ood6n1vrh4up-6{list-style-type:none}ul.lst-kix_ood6n1vrh4up-5{list-style-type:none}.lst-kix_jsy652pf3qrm-5>li:before{content:"\0025a0   "}ul.lst-kix_ood6n1vrh4up-8{list-style-type:none}ul.lst-kix_ood6n1vrh4up-7{list-style-type:none}.lst-kix_pugfjjme5q-0>li:before{content:"\0025cf   "}ul.lst-kix_qvl8ngt12rhu-7{list-style-type:none}ul.lst-kix_qvl8ngt12rhu-8{list-style-type:none}ul.lst-kix_qvl8ngt12rhu-5{list-style-type:none}ul.lst-kix_qvl8ngt12rhu-6{list-style-type:none}ul.lst-kix_qvl8ngt12rhu-3{list-style-type:none}.lst-kix_jsy652pf3qrm-7>li:before{content:"\0025cb   "}ul.lst-kix_qvl8ngt12rhu-4{list-style-type:none}ul.lst-kix_qvl8ngt12rhu-1{list-style-type:none}ul.lst-kix_qvl8ngt12rhu-2{list-style-type:none}ul.lst-kix_qvl8ngt12rhu-0{list-style-type:none}.lst-kix_pugfjjme5q-6>li:before{content:"\0025cf   "}ul.lst-kix_tp8dcj8chtv6-8{list-style-type:none}.lst-kix_v612ti81tkyd-1>li:before{content:"\0025cb   "}ul.lst-kix_tp8dcj8chtv6-7{list-style-type:none}.lst-kix_pugfjjme5q-4>li:before{content:"\0025cb   "}ul.lst-kix_tp8dcj8chtv6-4{list-style-type:none}ul.lst-kix_tp8dcj8chtv6-3{list-style-type:none}.lst-kix_bwz1irxjtfi9-6>li:before{content:"\0025a0   "}ul.lst-kix_tp8dcj8chtv6-6{list-style-type:none}ul.lst-kix_tp8dcj8chtv6-5{list-style-type:none}.lst-kix_pugfjjme5q-2>li:before{content:"\0025a0   "}ul.lst-kix_tp8dcj8chtv6-0{list-style-type:none}ul.lst-kix_tp8dcj8chtv6-2{list-style-type:none}ul.lst-kix_tp8dcj8chtv6-1{list-style-type:none}.lst-kix_v612ti81tkyd-3>li:before{content:"\0025cf   "}.lst-kix_bwz1irxjtfi9-4>li:before{content:"\0025c6   "}ol.lst-kix_jj6p90k4m6me-6.start{counter-reset:lst-ctn-kix_jj6p90k4m6me-6 0}.lst-kix_t24wwq61hl5b-2>li:before{content:"\0025a0   "}.lst-kix_zbes9rxb87oe-3>li:before{content:"\0025cf   "}.lst-kix_t24wwq61hl5b-1>li:before{content:"\0025cb   "}ul.lst-kix_pugfjjme5q-7{list-style-type:none}ul.lst-kix_pugfjjme5q-8{list-style-type:none}ul.lst-kix_pugfjjme5q-5{list-style-type:none}.lst-kix_zbes9rxb87oe-4>li:before{content:"\0025cb   "}.lst-kix_jj6p90k4m6me-6>li{counter-increment:lst-ctn-kix_jj6p90k4m6me-6}ul.lst-kix_pugfjjme5q-6{list-style-type:none}ul.lst-kix_pugfjjme5q-3{list-style-type:none}.lst-kix_pugfjjme5q-8>li:before{content:"\0025a0   "}ul.lst-kix_pugfjjme5q-4{list-style-type:none}ul.lst-kix_pugfjjme5q-1{list-style-type:none}ul.lst-kix_pugfjjme5q-2{list-style-type:none}ul.lst-kix_pugfjjme5q-0{list-style-type:none}.lst-kix_4d87yzwg7veb-1>li:before{content:"\0025cb   "}.lst-kix_4d87yzwg7veb-2>li:before{content:"\0025a0   "}.lst-kix_zbes9rxb87oe-8>li:before{content:"\0025a0   "}.lst-kix_zbes9rxb87oe-7>li:before{content:"\0025cb   "}.lst-kix_ood6n1vrh4up-5>li:before{content:"\0025a0   "}.lst-kix_ci1ll0e8g7yl-7>li:before{content:"\0025cf   "}ul.lst-kix_4d87yzwg7veb-5{list-style-type:none}ul.lst-kix_4d87yzwg7veb-6{list-style-type:none}ul.lst-kix_4d87yzwg7veb-7{list-style-type:none}ul.lst-kix_4d87yzwg7veb-8{list-style-type:none}.lst-kix_ci1ll0e8g7yl-4>li:before{content:"\0025cf   "}.lst-kix_ci1ll0e8g7yl-8>li:before{content:"\0025cf   "}.lst-kix_ood6n1vrh4up-1>li:before{content:"\0025cb   "}.lst-kix_ci1ll0e8g7yl-3>li:before{content:"\0025cf   "}.lst-kix_ood6n1vrh4up-2>li:before{content:"\0025a0   "}.lst-kix_4d87yzwg7veb-5>li:before{content:"\0025a0   "}.lst-kix_4d87yzwg7veb-6>li:before{content:"\0025cf   "}.lst-kix_t24wwq61hl5b-6>li:before{content:"\0025cf   "}.lst-kix_t24wwq61hl5b-5>li:before{content:"\0025a0   "}.lst-kix_zbes9rxb87oe-0>li:before{content:"\0027a2   "}.lst-kix_tp8dcj8chtv6-8>li:before{content:"\0025a0   "}.lst-kix_ood6n1vrh4up-6>li:before{content:"\0025cf   "}.lst-kix_42om2jlkt9i7-2>li:before{content:"\0025a0   "}.lst-kix_42om2jlkt9i7-1>li:before{content:"\0025cb   "}ul.lst-kix_qebx5z30856i-0{list-style-type:none}.lst-kix_b3xcjmtuwb81-3>li:before{content:"\0025cf   "}.lst-kix_b3xcjmtuwb81-7>li:before{content:"\0025cb   "}ul.lst-kix_qebx5z30856i-2{list-style-type:none}ul.lst-kix_qebx5z30856i-1{list-style-type:none}ul.lst-kix_qebx5z30856i-4{list-style-type:none}ul.lst-kix_qebx5z30856i-3{list-style-type:none}.lst-kix_qvl8ngt12rhu-6>li:before{content:"\0025cf   "}ul.lst-kix_datxsq12qj0c-8{list-style-type:none}.lst-kix_tp8dcj8chtv6-5>li:before{content:"\0025a0   "}ul.lst-kix_datxsq12qj0c-7{list-style-type:none}.lst-kix_b3xcjmtuwb81-4>li:before{content:"\0025cb   "}ul.lst-kix_datxsq12qj0c-6{list-style-type:none}.lst-kix_qvl8ngt12rhu-7>li:before{content:"\0025cb   "}.lst-kix_tp8dcj8chtv6-4>li:before{content:"\0025cb   "}ul.lst-kix_datxsq12qj0c-5{list-style-type:none}ul.lst-kix_s719nfpkgkn3-8{list-style-type:none}ul.lst-kix_datxsq12qj0c-4{list-style-type:none}ul.lst-kix_s719nfpkgkn3-7{list-style-type:none}ul.lst-kix_datxsq12qj0c-3{list-style-type:none}ul.lst-kix_s719nfpkgkn3-6{list-style-type:none}ul.lst-kix_datxsq12qj0c-2{list-style-type:none}ul.lst-kix_s719nfpkgkn3-5{list-style-type:none}ul.lst-kix_datxsq12qj0c-1{list-style-type:none}ul.lst-kix_s719nfpkgkn3-4{list-style-type:none}ul.lst-kix_datxsq12qj0c-0{list-style-type:none}ul.lst-kix_s719nfpkgkn3-3{list-style-type:none}ul.lst-kix_s719nfpkgkn3-2{list-style-type:none}ul.lst-kix_s719nfpkgkn3-1{list-style-type:none}.lst-kix_ci1ll0e8g7yl-0>li:before{content:"\0025cf   "}ul.lst-kix_s719nfpkgkn3-0{list-style-type:none}ul.lst-kix_4d87yzwg7veb-1{list-style-type:none}ul.lst-kix_4d87yzwg7veb-2{list-style-type:none}ul.lst-kix_4d87yzwg7veb-3{list-style-type:none}ul.lst-kix_4d87yzwg7veb-4{list-style-type:none}.lst-kix_tp8dcj8chtv6-1>li:before{content:"\0025cb   "}.lst-kix_b3xcjmtuwb81-0>li:before{content:"\0025cf   "}.lst-kix_tp8dcj8chtv6-0>li:before{content:"\0025cf   "}ul.lst-kix_4d87yzwg7veb-0{list-style-type:none}.lst-kix_qvl8ngt12rhu-2>li:before{content:"\0025a0   "}ul.lst-kix_vxf40eumpq0q-0{list-style-type:none}ul.lst-kix_vxf40eumpq0q-1{list-style-type:none}.lst-kix_qvl8ngt12rhu-3>li:before{content:"\0025cf   "}.lst-kix_3arvx8vp4edq-8>li:before{content:"\0025a0   "}.lst-kix_e78lefxl84ok-8>li:before{content:"\0025a0   "}.lst-kix_v612ti81tkyd-6>li:before{content:"\0025cf   "}.lst-kix_phb4rwukmkow-4>li:before{content:"\0025cb   "}.lst-kix_phb4rwukmkow-8>li:before{content:"\0025a0   "}.lst-kix_esb2ihnubxs-0>li:before{content:"  "}ul.lst-kix_3xcyqiocqw8e-8{list-style-type:none}ul.lst-kix_3xcyqiocqw8e-7{list-style-type:none}.lst-kix_3arvx8vp4edq-4>li:before{content:"\0025cb   "}.lst-kix_datxsq12qj0c-3>li:before{content:"\0025cf   "}.lst-kix_6rbjdsr9r5fa-3>li:before{content:"\0025cf   "}.lst-kix_3arvx8vp4edq-0>li:before{content:"\0027a2   "}.lst-kix_phb4rwukmkow-0>li:before{content:"\0025cf   "}.lst-kix_esb2ihnubxs-4>li:before{content:"\0025cb   "}.lst-kix_esb2ihnubxs-8>li:before{content:"\0025a0   "}ul.lst-kix_phb4rwukmkow-7{list-style-type:none}ul.lst-kix_phb4rwukmkow-6{list-style-type:none}.lst-kix_6rbjdsr9r5fa-7>li:before{content:"\0025cb   "}ul.lst-kix_phb4rwukmkow-8{list-style-type:none}.lst-kix_vxf40eumpq0q-3>li:before{content:"\0025cf   "}ul.lst-kix_phb4rwukmkow-1{list-style-type:none}ul.lst-kix_phb4rwukmkow-0{list-style-type:none}ul.lst-kix_phb4rwukmkow-3{list-style-type:none}ul.lst-kix_phb4rwukmkow-2{list-style-type:none}ul.lst-kix_phb4rwukmkow-5{list-style-type:none}ul.lst-kix_phb4rwukmkow-4{list-style-type:none}.lst-kix_42om2jlkt9i7-5>li:before{content:"\0025a0   "}.lst-kix_w2nv1w5i9k8v-2>li:before{content:"\0025a0   "}.lst-kix_jj6p90k4m6me-8>li{counter-increment:lst-ctn-kix_jj6p90k4m6me-8}.lst-kix_jsy652pf3qrm-0>li:before{content:"\0025cf   "}.lst-kix_w2nv1w5i9k8v-6>li:before{content:"\0025cf   "}.lst-kix_9xd58wc8mdpn-6>li:before{content:"\0025cf   "}.lst-kix_jsy652pf3qrm-4>li:before{content:"\0025cb   "}.lst-kix_vxf40eumpq0q-7>li:before{content:"\0025cb   "}ul.lst-kix_jsy652pf3qrm-7{list-style-type:none}ul.lst-kix_jsy652pf3qrm-8{list-style-type:none}.lst-kix_3zd8jpb99zva-1>li:before{content:"\0025cb   "}.lst-kix_3zd8jpb99zva-5>li:before{content:"\0025a0   "}.lst-kix_53s3nxw2a9uy-5>li:before{content:"\0025cf   "}.lst-kix_jj6p90k4m6me-3>li{counter-increment:lst-ctn-kix_jj6p90k4m6me-3}ul.lst-kix_jsy652pf3qrm-3{list-style-type:none}.lst-kix_9xd58wc8mdpn-2>li:before{content:"\0025a0   "}ul.lst-kix_jsy652pf3qrm-4{list-style-type:none}ul.lst-kix_jsy652pf3qrm-5{list-style-type:none}ul.lst-kix_jsy652pf3qrm-6{list-style-type:none}ul.lst-kix_jsy652pf3qrm-0{list-style-type:none}ul.lst-kix_jsy652pf3qrm-1{list-style-type:none}ul.lst-kix_jsy652pf3qrm-2{list-style-type:none}.lst-kix_pugfjjme5q-1>li:before{content:"\0025cb   "}.lst-kix_53s3nxw2a9uy-1>li:before{content:"\0025cf   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_jsy652pf3qrm-8>li:before{content:"\0025a0   "}ul.lst-kix_e78lefxl84ok-8{list-style-type:none}ul.lst-kix_e78lefxl84ok-6{list-style-type:none}ul.lst-kix_e78lefxl84ok-7{list-style-type:none}.lst-kix_v612ti81tkyd-2>li:before{content:"\0025a0   "}.lst-kix_pugfjjme5q-5>li:before{content:"\0025a0   "}.lst-kix_bwz1irxjtfi9-5>li:before{content:"\0027a2   "}ul.lst-kix_e78lefxl84ok-0{list-style-type:none}ul.lst-kix_e78lefxl84ok-1{list-style-type:none}.lst-kix_datxsq12qj0c-7>li:before{content:"\0025cb   "}ul.lst-kix_e78lefxl84ok-4{list-style-type:none}ul.lst-kix_e78lefxl84ok-5{list-style-type:none}ul.lst-kix_e78lefxl84ok-2{list-style-type:none}ul.lst-kix_e78lefxl84ok-3{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c10{border-right-style:solid;border-top-width:0pt;border-right-width:0pt;padding-left:0pt;border-left-width:0pt;border-top-style:solid;margin-left:9pt;border-left-style:solid;border-bottom-width:0pt;border-bottom-style:solid;padding-right:0pt}.c12{border-right-style:solid;border-top-width:0pt;border-right-width:0pt;padding-left:0pt;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;border-bottom-style:solid;padding-right:0pt}.c1{background-color:#ffffff;color:#434343;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Cambria";font-style:normal}.c5{background-color:#ffffff;color:#434343;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Montserrat";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Cambria";font-style:normal}.c28{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c3{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Cambria";font-style:normal}.c39{color:#ff9900;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Montserrat";font-style:normal}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Montserrat";font-style:normal}.c34{color:#6b6b6b;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Montserrat";font-style:normal}.c21{color:#ff9900;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Montserrat";font-style:normal}.c2{background-color:#ffffff;padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c50{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c87{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Arial";font-style:normal}.c45{color:#e06666;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Montserrat";font-style:normal}.c78{padding-top:11pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c36{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Arial";font-style:normal}.c43{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c68{padding-top:12pt;padding-bottom:2pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c61{padding-top:0pt;padding-bottom:28pt;line-height:2.25;orphans:2;widows:2;text-align:left}.c66{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c56{padding-top:14pt;padding-bottom:4pt;line-height:1.2;orphans:2;widows:2;text-align:left}.c11{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c29{padding-top:0pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c38{padding-top:0pt;padding-bottom:0pt;line-height:0.5;orphans:2;widows:2;text-align:left}.c27{padding-top:0pt;padding-bottom:0pt;line-height:2.25;orphans:2;widows:2;text-align:left}.c17{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c69{padding-top:12pt;padding-bottom:12pt;line-height:0.5;orphans:2;widows:2;text-align:left}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c30{color:#434343;text-decoration:none;vertical-align:baseline;font-size:11.5pt;font-style:normal}.c14{color:#28dada;text-decoration:none;vertical-align:baseline;font-size:13pt;font-style:normal}.c16{color:#333333;text-decoration:none;vertical-align:baseline;font-size:11.5pt;font-style:normal}.c85{color:#e06666;text-decoration:none;vertical-align:baseline;font-size:16pt;font-style:normal}.c60{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11.5pt;font-style:normal}.c82{color:#ff9900;text-decoration:none;vertical-align:baseline;font-size:15pt;font-style:normal}.c24{color:#e06666;text-decoration:none;vertical-align:baseline;font-size:14pt;font-style:normal}.c86{color:#ff9900;text-decoration:none;vertical-align:baseline;font-size:16pt;font-style:normal}.c79{color:#ea9999;text-decoration:none;vertical-align:baseline;font-size:24pt;font-style:normal}.c77{color:#ff9900;text-decoration:none;vertical-align:baseline;font-size:21pt;font-style:normal}.c44{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c32{color:#bf9000;text-decoration:none;vertical-align:baseline;font-size:14pt;font-style:normal}.c71{color:#28dada;text-decoration:none;vertical-align:baseline;font-style:normal}.c46{color:#bf9000;text-decoration:none;vertical-align:baseline;font-style:normal}.c33{color:#1c1c1c;text-decoration:none;vertical-align:baseline;font-style:normal}.c54{color:#434343;text-decoration:none;vertical-align:baseline;font-style:normal}.c41{color:#383838;text-decoration:none;vertical-align:baseline;font-style:normal}.c89{text-decoration:none;vertical-align:baseline;font-style:normal}.c73{font-weight:400;font-size:10.5pt;font-family:"Montserrat"}.c7{font-size:12pt;font-weight:700;font-family:"Cambria"}.c70{color:#ff9900;font-size:19pt;font-weight:700}.c8{font-size:12pt;font-weight:400;font-family:"Cambria"}.c40{padding:0;margin:0}.c18{margin-left:9pt;padding-left:0pt}.c65{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c13{font-weight:700;font-family:"Cambria"}.c42{font-weight:400;font-family:"Cambria"}.c26{margin-left:0pt;padding-left:0pt}.c49{font-size:13.5pt;color:#434343}.c53{font-weight:700;font-family:"Montserrat"}.c22{margin-left:13.5pt;padding-left:0pt}.c35{font-size:14pt;color:#434343}.c55{color:inherit;text-decoration:inherit}.c72{font-weight:700;font-family:"Arial"}.c81{font-size:18pt;color:#e06666}.c83{color:#383838;font-weight:700}.c58{font-size:14pt;color:#e06666}.c23{margin-left:36pt;padding-left:0pt}.c52{margin-left:31.5pt}.c67{margin-left:4.5pt}.c80{font-size:11pt}.c57{font-size:18.5pt}.c51{padding-left:0pt}.c76{margin-left:36pt}.c62{color:#bf9000}.c84{color:#383838}.c31{margin-left:9pt}.c37{margin-left:-13.5pt}.c63{font-size:14pt}.c9{background-color:#ffffff}.c25{margin-left:13.5pt}.c19{height:11pt}.c47{font-size:13pt}.c59{margin-left:22.5pt}.c48{margin-left:-4.5pt}.c15{margin-left:-9pt}.c74{color:#1c1c1c}.c88{color:#000000}.c75{text-indent:-9pt}.c64{font-size:12pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c9 c65 doc-content"><div><p class="c6"><span class="c43"></span></p></div><h2 class="c9 c78" id="h.awuh0hywn3y7"><span class="c54 c53 c57">What is Artificial Neural Network?</span></h2><p class="c11"><span class="c8">The term </span><span class="c7">&quot;Artificial neural network&quot;</span><span class="c0">&nbsp;refers to a biologically inspired sub-field of artificial intelligence modeled after the brain. An Artificial neural network is usually a computational network based on biological neural networks that construct the structure of the human brain. Similar to a human brain has neurons interconnected to each other, artificial neural networks also have neurons that are linked to each other in various layers of the networks. </span></p><p class="c11"><span class="c8">These neurons are known as nodes. Artificial neural network tutorial covers all the aspects related to the artificial neural network. In this tutorial, we will discuss ANNs, Adaptive resonance theory, Kohonen self-organizing map, Building blocks, unsupervised learning, Genetic algorithm, etc.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 482.50px; height: 251.20px;"><img alt="" src="images/image28.png" style="width: 482.50px; height: 251.20px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c29 c9"><span class="c60 c13">Input Layer:</span></p><p class="c29 c9"><span class="c13 c30">As the name suggests, it accepts inputs in several different formats provided by the programmer.</span></p><p class="c29 c9"><span class="c60 c13">Hidden Layer:</span></p><p class="c29 c9"><span class="c30 c13">The hidden layer presents in-between input and output layers. It performs all the calculations to find hidden features and patterns.</span></p><p class="c9 c29"><span class="c60 c13">Output Layer:</span></p><p class="c29 c9"><span class="c16 c13">The input goes through a series of transformations using the hidden layer, which finally results in output that is conveyed using this layer.</span></p><p class="c29 c9"><span class="c13 c16">The artificial neural network takes input and computes the weighted sum of the inputs and includes a bias. This computation is represented in the form of a transfer function</span></p><p class="c28 title" id="h.cc9fu8flzy61"><span class="c72 c79">Hidden Layer</span></p><p class="c11"><span class="c42">A </span><span class="c13">Hidden Layer</span><span class="c42">&nbsp;is a crucial component of an </span><span class="c13">Artificial Neural Network (ANN)</span><span class="c42">&nbsp;that lies </span><span class="c13">between</span><span class="c42">&nbsp;the </span><span class="c13">input layer</span><span class="c42">&nbsp;and the </span><span class="c13">output layer</span><span class="c42">. These layers </span><span class="c13">process</span><span class="c42">&nbsp;the input data by performing mathematical transformations using </span><span class="c13">weights, biases, and activation functions</span><span class="c42">, allowing the network to learn complex patterns and relationships.</span></p><p class="c28 title" id="h.nmoe0lp3hmf1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 528.50px; height: 364.87px;"><img alt="" src="images/image18.png" style="width: 528.50px; height: 364.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c28 title" id="h.t04lub3wnig9"><span class="c70"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13 c77">Types of &nbsp;Hidden Layer</span></p><p class="c6"><span class="c44 c42"></span></p><p class="c11"><span class="c24 c13">1.Dense Layer &nbsp;(Fully Connected Layer)</span></p><p class="c11"><span class="c64">&nbsp;</span></p><p class="c11"><span class="c8">A </span><span class="c7">Dense Layer</span><span class="c8">&nbsp;is the most common type of hidden layer in an </span><span class="c7">Artificial Neural Network (ANN)</span><span class="c8">. In this layer, </span><span class="c7">every neuron</span><span class="c8">&nbsp;is connected to </span><span class="c7">all neurons</span><span class="c8">&nbsp;in both the </span><span class="c7">previous</span><span class="c8">&nbsp;and </span><span class="c7">next</span><span class="c8">&nbsp;layers, making it a </span><span class="c7">fully connected</span><span class="c0">&nbsp;structure.</span></p><p class="c17"><span class="c0">This layer performs two key operations:</span></p><ol class="c40 lst-kix_jj6p90k4m6me-0 start" start="1"><li class="c17 c23 li-bullet-0"><span class="c7">Weighted Sum of Inputs</span><span class="c0">: Each neuron computes a weighted sum of all its inputs.</span></li><li class="c17 c23 li-bullet-0"><span class="c7">Activation Function</span><span class="c8">: A non-linear activation function (such as </span><span class="c7">ReLU, Sigmoid, or Tanh</span><span class="c0">) is applied to introduce non-linearity, allowing the network to learn complex patterns.</span></li></ol><p class="c11"><span class="c7">Role</span><span class="c0">: Learns high-level representations from input data.</span></p><p class="c11"><span class="c7">Function</span><span class="c0">: Performs a weighted sum of inputs followed by activation.</span></p><p class="c11"><span class="c7">Flexibility</span><span class="c0">: Works with various activation functions and optimizers to adapt to different learning tasks.</span></p><p class="c11"><span class="c7">Trainable Parameters</span><span class="c0">: Contains a large number of parameters (weights and biases), making it computationally expensive but powerful for learning.</span></p><p class="c17"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 322.67px;"><img alt="" src="images/image26.png" style="width: 624.00px; height: 322.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h4 class="c68" id="h.sonuheirznkv"><span class="c13 c88">Example</span><span class="c13 c44">:</span></h4><p class="c17"><span class="c8">In a </span><span class="c7">Natural Language Processing (NLP) model</span><span class="c8">, a </span><span class="c7">Dense Layer</span><span class="c8">&nbsp;is often used in the </span><span class="c7">final classification step</span><span class="c8">&nbsp;of a </span><span class="c7">sentiment analysis model</span><span class="c8">. After extracting text features using an </span><span class="c7">Embedding Layer</span><span class="c8">&nbsp;and </span><span class="c7">Recurrent Layers (like LSTMs)</span><span class="c8">, a Dense Layer with a </span><span class="c7">Softmax activation</span><span class="c8">&nbsp;predicts sentiment categories (e.g., </span><span class="c7">positive, neutral, negative</span><span class="c8">)</span><span class="c50">.</span></p><p class="c11"><span class="c24 c13">2. Convolutional Layer</span></p><p class="c11"><span class="c0">Convolutional Neural Networks (CNNs) are a specialized type of neural network that excels in tasks like image classification, object detection, and segmentation. They are similar to regular Neural Networks in that they consist of neurons with learnable weights and biases. Each neuron processes input through a dot product, followed by an optional non-linearity. Despite the differences in structure, the underlying goal remains the same: transforming raw input into differentiable outputs. For CNNs, this involves transforming raw image pixels into class scores or features.</span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 517.00px; height: 197.00px;"><img alt="" src="images/image4.png" style="width: 517.00px; height: 197.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c11"><span class="c7">Function: </span><span class="c0">These layers apply convolution using filters to scan the input and create feature maps. This helps in detecting patterns like edges and textures.</span></p><p class="c11"><span class="c7">Role: </span><span class="c0">Extracts spatial features from images by identifying patterns in data.</span></p><p class="c11"><span class="c7">Example</span><span class="c8">: In </span><span class="c7">object detection</span><span class="c8">, convolutional layers detect edges and shapes, and in </span><span class="c7">facial recognition</span><span class="c0">, they learn features like eyes and facial contours.</span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c13 c24">3. Recurrent Layer (RNN, LSTM, GRU)</span></p><p class="c17"><span class="c8">Recurrent layers, like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit), are used in Recurrent Neural Networks (RNNs) to handle sequential data such as time series or text. They have feedback loops that allow information to persist across time steps, making them ideal for tasks with context and temporal dependencies.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 336.50px; height: 286.00px;"><img alt="" src="images/image34.png" style="width: 356.61px; height: 291.15px; margin-left: -4.64px; margin-top: -5.15px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c69"><span class="c7">R</span><span class="c7">ole: </span><span class="c0">Processes sequential data with temporal dependencies.</span></p><p class="c69"><span class="c7">Function: </span><span class="c0">Maintains state across time steps.</span></p><p class="c38"><span class="c7">Example: </span><span class="c0">Used in language modeling and time series prediction</span></p><p class="c38 c19"><span class="c0"></span></p><p class="c19 c38"><span class="c0"></span></p><p class="c11"><span class="c24 c13">4. Dropout Layer</span></p><p class="c11"><span class="c7">Dropout layers</span><span class="c8">&nbsp;are a </span><span class="c7">regularization technique</span><span class="c8">&nbsp;used to </span><span class="c7">prevent overfitting</span><span class="c8">&nbsp;in neural networks. During training, a fraction of neurons are </span><span class="c7">randomly dropped</span><span class="c8">&nbsp;(set to zero) at each training step, which forces the network to learn </span><span class="c7">more robust features</span><span class="c8">&nbsp;and generalize better. The neurons that remain active are chosen with a probability </span><span class="c7">p</span><span class="c0">, and during training, the network essentially learns with a subset of neurons, making it harder for the model to memorize the data.</span></p><p class="c17"><span class="c8">This approach helps the model avoid overfitting by preventing it from becoming too reliant on specific neurons or connections. As a result, the network is encouraged to learn distributed, more </span><span class="c7">generalizable features</span><span class="c0">&nbsp;that perform better on unseen data.</span></p><p class="c17"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 264.00px;"><img alt="" src="images/image11.png" style="width: 624.00px; height: 264.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c7">Role</span><span class="c8">: </span><span class="c7">Prevents overfitting</span><span class="c0">&nbsp;by reducing model complexity.</span></p><p class="c4"><span class="c7">Function</span><span class="c8">: </span><span class="c7">Randomly drops neurons</span><span class="c0">&nbsp;during training to make the model less reliant on specific neurons.</span></p><p class="c4"><span class="c7">Example</span><span class="c8">: Often used in </span><span class="c7">deep learning models</span><span class="c8">&nbsp;(e.g., CNNs, RNNs) to improve </span><span class="c7">generalization</span><span class="c0">&nbsp;on unseen data.</span></p><p class="c6"><span class="c43"></span></p><p class="c11"><span class="c24 c13">5. Pooling Layer</span></p><p class="c11"><span class="c8">It is common to periodically insert a </span><span class="c7">Pooling Layer</span><span class="c8">&nbsp;between successive </span><span class="c7">Convolutional Layers</span><span class="c0">&nbsp;in a ConvNet architecture.</span></p><p class="c11"><span class="c8">The function of the pooling layer is to progressively reduce the </span><span class="c7">spatial size</span><span class="c0">&nbsp;of the feature maps, which helps in reducing the number of parameters and computation, thereby controlling overfitting.</span></p><p class="c11"><span class="c8">The </span><span class="c7">Pooling Layer</span><span class="c8">&nbsp;operates independently on every </span><span class="c7">depth slice</span><span class="c8">&nbsp;of the input and resizes it spatially using operations like </span><span class="c7">Max Pooling</span><span class="c8">&nbsp;or </span><span class="c7">Average Pooling</span><span class="c0">.</span></p><p class="c11"><span class="c8">The most common form of pooling is </span><span class="c7">Max Pooling</span><span class="c8">, where a </span><span class="c7">2&times;2 filter</span><span class="c8">&nbsp;with a </span><span class="c7">stride of 2</span><span class="c8">&nbsp;is applied to downsample every depth slice in the input by a factor of </span><span class="c7">2</span><span class="c8">&nbsp;along both width and height, effectively discarding </span><span class="c7">75%</span><span class="c0">&nbsp;of the activations.</span></p><p class="c11"><span class="c8">Every </span><span class="c7">Max operation</span><span class="c8">&nbsp;takes the </span><span class="c7">maximum value</span><span class="c8">&nbsp;from a small </span><span class="c7">2&times;2 region</span><span class="c0">&nbsp;in a depth slice, retaining only the most important features while reducing computational complexity.</span></p><p class="c6"><span class="c43"></span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 243.57px; height: 190.10px;"><img alt="" src="images/image12.png" style="width: 243.57px; height: 190.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 371.00px; height: 179.60px;"><img alt="" src="images/image13.png" style="width: 351.00px; height: 171.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c43"></span></p><p class="c11"><span class="c8">Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. </span><span class="c7">Left</span><span class="c8">: In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. </span><span class="c7">Right:</span><span class="c8">&nbsp;The most common downsampling operation is max, giving rise to max pooling, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square)</span><span class="c43">.</span></p><p class="c6"><span class="c43"></span></p><p class="c6"><span class="c43"></span></p><p class="c11"><span class="c24 c13">6. Batch Normalization Layer</span></p><p class="c17"><span class="c8">A </span><span class="c7">Batch Normalization (BN) Layer</span><span class="c8">&nbsp;normalizes the output of the previous activation layer by </span><span class="c7">subtracting the batch mean</span><span class="c8">&nbsp;and </span><span class="c7">dividing by the batch standard deviation</span><span class="c8">. This ensures that the activations of each layer have a </span><span class="c7">mean of zero</span><span class="c8">&nbsp;and a </span><span class="c7">standard deviation of one</span><span class="c8">. By doing so, it helps in </span><span class="c7">accelerating the training process</span><span class="c8">&nbsp;and </span><span class="c7">improving the overall performance</span><span class="c8">&nbsp;of the neural network. Batch normalization also reduces the risk of </span><span class="c7">vanishing/exploding gradients</span><span class="c8">&nbsp;and allows the network to use </span><span class="c7">higher learning rates</span><span class="c0">, which further speeds up training.</span></p><p class="c17"><span class="c8">The BN layer can be applied to </span><span class="c7">both convolutional</span><span class="c8">&nbsp;and </span><span class="c7">fully connected layers</span><span class="c8">, and it is typically followed by a </span><span class="c7">scaling and shifting step</span><span class="c0">, where learnable parameters (gamma and beta) are introduced to restore the representational power of the model after normalization</span></p><p class="c17"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 259.50px; height: 243.72px;"><img alt="" src="images/image7.png" style="width: 415.20px; height: 270.11px; margin-left: -71.86px; margin-top: -12.24px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 156.00px;"><img alt="" src="images/image33.png" style="width: 624.00px; height: 156.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c43"></span></p><p class="c6"><span class="c43"></span></p><p class="c6"><span class="c43"></span></p><hr><p class="c6"><span class="c43"></span></p><p class="c11"><span class="c24 c53">Activation Function </span></p><p class="c11"><span class="c0">An Activation Function decides whether a neuron should be activated or not. This means that it will decide whether the neuron&rsquo;s input to the network is important or not in the process of prediction using simpler mathematical operations. The role of the Activation Function is to derive output from a set of input values fed to a node (or a layer).</span></p><p class="c12 c2"><span class="c7">What exactly is a node?</span><span class="c8">&nbsp;Well, if we compare the neural network to our brain, a node is a replica of a neuron that receives a set of input signals&mdash;external stimuli. &nbsp;In</span><span class="c8"><a class="c55" href="https://www.google.com/url?q=https://www.v7labs.com/blog/deep-learning-guide&amp;sa=D&amp;source=editors&amp;ust=1738420484612483&amp;usg=AOvVaw2kyYR13I3U9jQHHZ_Fy39C">&nbsp;</a></span><span class="c0">Deep Learning &nbsp;this is also the role of the Activation Function&mdash;that&rsquo;s why it&rsquo;s often referred to as a Transfer Function in Artificial Neural Network.</span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c0">The primary role of the Activation Function is to transform the summed weighted input from the node into an output value to be fed to the next hidden layer or as output. </span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 414.50px; height: 281.78px;"><img alt="" src="images/image1.png" style="width: 490.79px; height: 367.34px; margin-left: -38.15px; margin-top: -41.60px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c24 c53"></span></p><p class="c11"><span class="c21">Different types of Activation function</span></p><p class="c6"><span class="c21"></span></p><p class="c11"><span class="c53 c58">Linear Activation Function</span></p><p class="c11"><span class="c0">Linear Activation Function resembles straight line define by y=x. No matter how many layers the neural network contains, if they all use linear activation functions, the output is a linear combination of the input.</span></p><p class="c11"><span class="c0">The range of the output spans from </span></p><p class="c11"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(&minus;&infin; to +&infin;) (&minus;&infin; to +&infin;)</span></p><p class="c11"><span class="c0">Linear activation function is used at just one place i.e. output layer.</span></p><p class="c11"><span class="c0">Using linear activation across all layers makes the network&rsquo;s ability to learn complex patterns limited.</span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c0">Linear activation functions are useful for specific tasks but must be combined with non-linear functions to enhance the neural network&rsquo;s learning and predictive capabilities.</span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 314.29px; height: 264.01px;"><img alt="" src="images/image37.png" style="width: 384.55px; height: 306.37px; margin-left: -38.82px; margin-top: -25.17px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c20"></span></p><p class="c6"><span class="c20"></span></p><p class="c6"><span class="c20"></span></p><p class="c6"><span class="c20"></span></p><p class="c6"><span class="c20"></span></p><p class="c6"><span class="c20"></span></p><p class="c6"><span class="c20"></span></p><p class="c11"><span class="c8">The linear activation function, also known as the identity function or &quot;no activation&quot;, is where the activation is directly proportional to the input. Mathematically, it is represented as &nbsp;</span><span class="c42 c47">&nbsp;</span><span class="c13 c14">&#119891;(&#119909;) = &#119909; </span></p><p class="c6"><span class="c14 c13"></span></p><p class="c11"><span class="c0">However, a linear activation function has two major problems :</span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c0">It&rsquo;s not possible to use backpropagation as the derivative of the function is a constant and has no relation to the input x. </span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c0">All layers of the neural network will collapse into one if a linear activation function is used. No matter the number of layers in the neural network, the last layer will still be a linear function of the first layer. So, essentially, a linear activation function turns the neural network into just one layer.</span></p><p class="c6"><span class="c53 c63 c71"></span></p><p class="c11"><span class="c24 c53">Non-Linear Activation Functions</span></p><p class="c11"><span class="c0">The linear activation function shown above is simply a linear regression model. </span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c0">Because of its limited power, this does not allow the model to create complex mappings between the network&rsquo;s inputs and outputs. </span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c0">Non-linear activation functions solve the following limitations of linear activation functions:</span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c0">They allow backpropagation because now the derivative function would be related to the input, and it&rsquo;s possible to go back and understand which weights in the input neurons can provide a better prediction.</span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c0">They allow the stacking of multiple layers of neurons as the output would now be a non-linear combination of input passed through multiple layers. Any output can be represented as a functional computation in a neural network</span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c46 c7">Sigmoid / Logistic Activation Function </span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c0">This function takes any real value as input and outputs values in the range of 0 to 1. </span></p><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c0">The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0, </span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 329.50px; height: 273.57px;"><img alt="" src="images/image27.png" style="width: 402.36px; height: 320.45px; margin-left: -36.11px; margin-top: -28.26px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 135.00px; height: 59.00px;"><img alt="" src="images/image20.png" style="width: 233.70px; height: 230.06px; margin-left: -48.50px; margin-top: -100.75px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 294.00px; height: 241.57px;"><img alt="" src="images/image10.png" style="width: 342.91px; height: 303.49px; margin-left: -24.18px; margin-top: -45.92px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c40 lst-kix_42om2jlkt9i7-0 start"><li class="c2 c26 li-bullet-0"><span class="c8">It is commonly used for models where we have to predict the probability as an output. Since probability of anything exists only between the </span><span class="c7 c62">range of 0 and 1</span><span class="c0">, sigmoid is the right choice because of its range.</span></li></ul><p class="c6 c9"><span class="c0"></span></p><ul class="c40 lst-kix_42om2jlkt9i7-0"><li class="c2 c26 li-bullet-0"><span class="c8">The function is differentiable and provides a smooth gradient, i.e., preventing jumps in output values. This is represented by an </span><span class="c7 c62">S-shape</span><span class="c0">&nbsp;of the sigmoid activation function. </span></li></ul><p class="c6 c9"><span class="c0"></span></p><ul class="c40 lst-kix_42om2jlkt9i7-0"><li class="c2 c26 li-bullet-0"><span class="c0">The sigmoid function&#39;s gradient is significant only between -3 and 3, becoming nearly zero beyond this range. This leads to the vanishing gradient problem, hindering learning. Additionally, its output is not symmetric around zero.</span></li></ul><p class="c6"><span class="c0"></span></p><p class="c11"><span class="c32 c13">Tanh Function (Hyperbolic Tangent)</span></p><p class="c11"><span class="c0">Tanh function is very similar to the sigmoid/logistic activation function, and even has the same S-shape with the difference in output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 320.50px; height: 308.59px;"><img alt="" src="images/image8.png" style="width: 380.94px; height: 328.07px; margin-left: -30.52px; margin-top: -19.48px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 302.50px; height: 275.75px;"><img alt="" src="images/image24.png" style="width: 355.48px; height: 313.20px; margin-left: -25.07px; margin-top: -23.83px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c3"></span></p><p class="c6"><span class="c3"></span></p><p class="c6"><span class="c3"></span></p><p class="c6"><span class="c3"></span></p><p class="c6"><span class="c3"></span></p><p class="c6"><span class="c3"></span></p><p class="c6"><span class="c3"></span></p><p class="c6"><span class="c3"></span></p><p class="c6"><span class="c3"></span></p><ul class="c40 lst-kix_datxsq12qj0c-0 start"><li class="c2 c15 c51 li-bullet-0"><span class="c0">The output of the tanh activation function is Zero centered; hence we can easily map the output values as strongly negative, neutral, or strongly positive.</span></li><li class="c2 c51 c15 li-bullet-0"><span class="c8">Usually used in hidden layers of a neural network as its values lie between </span><span class="c8 c62">-1</span><span class="c0">&nbsp;to; therefore, the mean for the hidden layer comes out to be 0 or very close to it. It helps in centering the data and makes learning for the next layer much easier.</span></li><li class="c2 c51 c15 li-bullet-0"><span class="c42 c9 c47 c74">It also faces the problem of vanishing gradients similar to the sigmoid activation function. Plus the gradient of the tanh function is much steeper as compared to the sigmoid function.</span></li></ul><p class="c6 c9 c15"><span class="c0"></span></p><p class="c2 c15"><span class="c13 c62 c63">ReLU Function </span><span class="c42 c62 c63">(</span><span class="c8 c46">Rectified Linear Unit)</span></p><p class="c6 c9 c15"><span class="c0"></span></p><p class="c2 c15"><span class="c0">Although it gives an impression of a linear function, ReLU has a derivative function and allows for backpropagation while simultaneously making it computationally efficient. </span></p><p class="c6 c9 c15"><span class="c0"></span></p><p class="c2 c15"><span class="c0">The main catch here is that the ReLU function does not activate all the neurons at the same time. The neurons will only be deactivated if the output of the linear transformation is less than 0</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 321.50px; height: 296.61px;"><img alt="" src="images/image9.png" style="width: 381.40px; height: 335.92px; margin-left: -28.73px; margin-top: -23.27px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 322.50px; height: 298.25px;"><img alt="" src="images/image15.png" style="width: 378.27px; height: 334.62px; margin-left: -30.31px; margin-top: -23.64px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 120.00px; height: 22.34px;"><img alt="" src="images/image32.png" style="width: 173.74px; height: 135.88px; margin-left: -26.17px; margin-top: -69.94px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c12 c2"><span class="c0">The advantages of using ReLU as an activation function:<br></span></p><ul class="c40 lst-kix_qebx5z30856i-0 start"><li class="c2 c51 c15 li-bullet-0"><span class="c0">Since only a certain number of neurons are activated, the ReLU function is far more computationally efficient when compared to the sigmoid and tanh functions.</span></li><li class="c2 c51 c15 li-bullet-0"><span class="c8">ReLU accelerates the convergence of gradient descent towards the global minimum of the </span><span class="c8">loss function</span><span class="c0">&nbsp;due to its linear, non-saturating property.</span></li></ul><p class="c6 c9 c15"><span class="c46 c13 c80"></span></p><ul class="c40 lst-kix_qebx5z30856i-0"><li class="c12 c2 c15 li-bullet-0"><span class="c0">The negative side of the graph makes the gradient value zero. Due to this reason, during the backpropagation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated. </span></li><li class="c2 c51 c15 li-bullet-0"><span class="c0">All the negative input values become zero immediately, which decreases the model&rsquo;s ability to fit or train from the data properly. </span></li></ul><p class="c6 c9 c15"><span class="c0"></span></p><p class="c2 c15"><span class="c32 c13">Leaky ReLU Function</span></p><p class="c2 c15"><span class="c0">Leaky ReLU is an improved version of ReLU function to solve the Dying ReLU problem as it has a small positive slope in the negative area.</span></p><p class="c2 c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 333.31px; height: 304.76px;"><img alt="" src="images/image30.png" style="width: 387.31px; height: 342.82px; margin-left: -34.76px; margin-top: -21.45px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 188.49px; height: 29.76px;"><img alt="" src="images/image21.png" style="width: 260.21px; height: 182.30px; margin-left: -35.03px; margin-top: -93.30px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 309.19px; height: 273.24px;"><img alt="" src="images/image29.png" style="width: 351.43px; height: 308.84px; margin-left: -27.03px; margin-top: -22.17px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c9 c15"><span class="c0"></span></p><ul class="c40 lst-kix_3xcyqiocqw8e-0 start"><li class="c2 c22 li-bullet-0"><span class="c0">The advantages of Leaky ReLU are same as that of ReLU, in addition to the fact that it does enable backpropagation, even for negative input values. </span></li><li class="c2 c22 li-bullet-0"><span class="c0">By making this minor modification for negative input values, the gradient of the left side of the graph comes out to be a non-zero value. Therefore, we would no longer encounter dead neurons in that region.</span></li></ul><p class="c6 c9 c15"><span class="c0"></span></p><p class="c2 c15"><span class="c0">The limitations that this function faces include:</span></p><ul class="c40 lst-kix_9xd58wc8mdpn-0 start"><li class="c12 c2 c25 li-bullet-0"><span class="c0">The predictions may not be consistent for negative input values. </span></li><li class="c12 c2 c25 li-bullet-0"><span class="c0">The gradient for negative values is a small value that makes the learning of model parameters time-consuming.</span></li></ul><p class="c12 c6 c37 c9"><span class="c0"></span></p><p class="c12 c2 c37"><span class="c32 c13">Gaussian Error Linear Unit (GELU)</span></p><ul class="c40 lst-kix_v612ti81tkyd-0 start"><li class="c10 c4 c9 li-bullet-0"><span class="c0">The Gaussian Error Linear Unit (GELU) activation function is compatible with BERT, ROBERTa, ALBERT, and other top NLP models. This activation function is motivated by combining properties from dropout, zoneout, and ReLUs. </span></li><li class="c10 c4 c9 li-bullet-0"><span class="c0">GELU combines properties of dropout, zoneout, and ReLU.</span></li></ul><p class="c10 c4 c9 c19"><span class="c0"></span></p><ul class="c40 lst-kix_v612ti81tkyd-0"><li class="c4 c9 c10 li-bullet-0"><span class="c0">It stochastically multiplies the input by zero or one, based on the input value.</span></li></ul><p class="c10 c4 c9 c19"><span class="c0"></span></p><ul class="c40 lst-kix_v612ti81tkyd-0"><li class="c10 c4 c9 li-bullet-0"><span class="c0">Uses a Bernoulli distribution with the cumulative distribution function of a standard normal distribution (&Phi;(x)).</span></li></ul><p class="c10 c4 c9 c19"><span class="c0"></span></p><ul class="c40 lst-kix_v612ti81tkyd-0"><li class="c10 c4 c9 li-bullet-0"><span class="c0">This approach is effective in models like BERT, ROBERTa, and ALBERT.</span></li></ul><p class="c10 c4 c9 c19"><span class="c0"></span></p><ul class="c40 lst-kix_v612ti81tkyd-0"><li class="c10 c4 c9 li-bullet-0"><span class="c0">Neuron inputs tend to follow a normal distribution, especially with Batch Normalization.</span></li></ul><p class="c12 c4 c9 c19"><span class="c0"></span></p><p class="c4 c9 c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 334.50px; height: 299.75px;"><img alt="" src="images/image17.png" style="width: 385.82px; height: 332.45px; margin-left: -22.26px; margin-top: -13.87px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 343.50px; height: 62.70px;"><img alt="" src="images/image39.png" style="width: 405.19px; height: 178.94px; margin-left: -33.12px; margin-top: -67.15px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c15"><span class="c8 c9">GELU nonlinearity is better than ReLU and ELU activations and finds performance improvements across all tasks in domains of </span><span class="c8 c9">computer vision</span><span class="c0 c9">, natural language processing, and speech recognition.</span></p><p class="c6 c9 c15"><span class="c33 c42 c9 c47"></span></p><p class="c2 c15"><span class="c13 c9 c32">&zwj;Softmax Function</span></p><p class="c6 c9 c15"><span class="c32 c13 c9"></span></p><p class="c2 c15"><span class="c0 c9">Before exploring the ins and outs of the Softmax activation function, we should focus on its building block&mdash;the sigmoid/logistic activation function that works on calculating probability values. </span></p><ul class="c40 lst-kix_3arvx8vp4edq-0 start"><li class="c12 c2 c48 li-bullet-0"><span class="c0 c9">Sigmoid outputs values between 0 and 1, representing probabilities, but doesn&#39;t ensure the sum of all class probabilities equals 1.</span></li><li class="c12 c2 c48 li-bullet-0"><span class="c0 c9">Softmax is a combination of multiple sigmoids, calculating relative probabilities for each class.</span></li><li class="c12 c2 c48 li-bullet-0"><span class="c0 c9">It ensures the sum of all probabilities equals 1.</span></li><li class="c12 c2 c48 li-bullet-0"><span class="c0 c9">Softmax is commonly used as the activation function in the final layer of a neural network for multi-class classification.</span></li></ul><p class="c2 c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 316.50px; height: 280.38px;"><img alt="" src="images/image5.png" style="width: 367.78px; height: 317.49px; margin-left: -28.88px; margin-top: -20.03px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 273.65px; height: 75.23px;"><img alt="" src="images/image16.png" style="width: 340.15px; height: 255.62px; margin-left: -33.80px; margin-top: -92.31px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><hr><p class="c6 c9 c15"><span class="c45 c9"></span></p><p class="c6 c9 c15"><span class="c9 c45"></span></p><p class="c2 c15"><span class="c45 c9">Optimizers </span></p><p class="c2 c15"><span class="c0 c9">In deep learning, optimizers are crucial as algorithms that dynamically fine-tune a model&rsquo;s parameters throughout the training process, aiming to minimize a predefined loss function. These specialized algorithms facilitate the learning process of neural networks by iteratively refining the weights and biases based on the feedback received from the data. Well-known optimizers in deep learning encompass Stochastic Gradient Descent (SGD), Adam, and RMSprop, each equipped with distinct update rules, learning rates, and momentum strategies, all geared towards the overarching goal of discovering and converging upon optimal model parameters, thereby enhancing overall performance.</span></p><p class="c6 c9 c15"><span class="c0 c9"></span></p><p class="c2 c15"><span class="c9 c39">Types of Optimizer</span></p><p class="c6 c9 c15"><span class="c0 c9"></span></p><ul class="c40 lst-kix_bwz1irxjtfi9-0 start"><li class="c2 c52 c51 li-bullet-0"><span class="c0 c9">Gradient Descent</span></li><li class="c2 c52 c51 li-bullet-0"><span class="c0 c9">Stochastic Gradient Descent</span></li><li class="c2 c51 c52 li-bullet-0"><span class="c0 c9">Adagrad</span></li><li class="c2 c52 c51 li-bullet-0"><span class="c0 c9">Adadelta</span></li><li class="c2 c52 c51 li-bullet-0"><span class="c0 c9">RMSprop</span></li><li class="c2 c52 c51 li-bullet-0"><span class="c0 c9">Adam</span></li></ul><p class="c6 c9 c15"><span class="c33 c13 c9 c47"></span></p><p class="c2 c15"><span class="c54 c13 c9 c47">Gradient Descent :</span></p><ul class="c40 lst-kix_s719nfpkgkn3-0 start"><li class="c2 c51 c67 li-bullet-0"><span class="c33 c8 c9">Gradient Descent is an optimization algorithm used to find the local minimum of a differentiable function.It minimizes a cost function by adjusting the parameters (coefficients) of the function.</span></li><li class="c2 c67 c51 li-bullet-0"><span class="c33 c8 c9">It is widely used in neural networks, particularly for convex optimization problems.</span></li><li class="c2 c67 c51 li-bullet-0"><span class="c33 c8 c9">The algorithm updates the weights by moving in the direction of the negative gradient (downhill) to minimize the cost function.</span></li><li class="c2 c67 c51 li-bullet-0"><span class="c8 c9 c33">It aims to find the best-suited parameter values that correspond to the global minima.</span></li><li class="c2 c67 c51 li-bullet-0"><span class="c33 c8 c9">Weights are adjusted based on the slope: a negative slope increases the weight, while a positive slope reduces it.</span></li></ul><p class="c2 c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 437.50px; height: 327.05px;"><img alt="" src="images/image25.gif" style="width: 437.50px; height: 327.05px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c9 c15"><span class="c33 c42 c9 c47"></span></p><p class="c2 c15"><span class="c54 c9 c73">Finding the global minima for a convex optimization problem using Gradient Descent</span></p><p class="c6 c9"><span class="c54 c73 c9"></span></p><p class="c2 c15"><span class="c1">Stochastic Gradient Descent</span></p><p class="c6 c9"><span class="c1"></span></p><ul class="c40 lst-kix_3zd8jpb99zva-0 start"><li class="c2 c18 li-bullet-0"><span class="c0 c9">Stochastic Gradient Descent (SGD) is a variant of Gradient Descent designed to handle non-convex optimization problems.</span></li></ul><p class="c6 c31 c9"><span class="c0 c9"></span></p><ul class="c40 lst-kix_3zd8jpb99zva-0"><li class="c2 c18 li-bullet-0"><span class="c0 c9">It addresses the issue of local minima by updating weights one at a time, instead of processing in batches.</span></li></ul><p class="c6 c9 c31"><span class="c0 c9"></span></p><ul class="c40 lst-kix_3zd8jpb99zva-0"><li class="c2 c18 li-bullet-0"><span class="c8 c9">This approach is faster and minimizes the cost function after each iteration </span><span class="c3 c9">(epoch).</span></li></ul><p class="c6 c31 c9"><span class="c0 c9"></span></p><ul class="c40 lst-kix_3zd8jpb99zva-0"><li class="c2 c18 li-bullet-0"><span class="c0 c9">Frequent updates with high variance allow the gradient to jump to potential global minima, but can cause fluctuations in the cost function.</span></li></ul><p class="c6 c31 c9"><span class="c0 c9"></span></p><ul class="c40 lst-kix_3zd8jpb99zva-0"><li class="c2 c18 li-bullet-0"><span class="c0 c9">Choosing a small learning rate can lead to slow convergence, while a large learning rate can cause poor convergence, with the cost function fluctuating or diverging.</span></li></ul><p class="c2 c31"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 329.50px; height: 238.54px;"><img alt="" src="images/image2.gif" style="width: 329.50px; height: 238.54px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c9 c34">Converging at the global minima using SGD for non-convex data</span></p><p class="c6 c9"><span class="c34 c9"></span></p><p class="c2 c37"><span class="c13 c9 c35">Adagrad (</span><span class="c42 c9 c49">Adaptive Gradient</span><span class="c1">)</span></p><p class="c6 c9"><span class="c1"></span></p><ul class="c40 lst-kix_qvl8ngt12rhu-0 start"><li class="c2 c18 li-bullet-0"><span class="c0 c9">Adagrad is an adaptive gradient optimization algorithm where the learning rate varies for each parameter.</span></li><li class="c2 c18 li-bullet-0"><span class="c0 c9">Unlike Stochastic Gradient Descent (SGD), it uses a different learning rate for each iteration (epoch).</span></li><li class="c2 c18 li-bullet-0"><span class="c0 c9">It performs smaller updates for weights corresponding to high-frequency features and larger updates for low-frequency features, improving performance and accuracy.</span></li><li class="c2 c18 li-bullet-0"><span class="c0 c9">Adagrad is especially effective for sparse data.</span></li><li class="c2 c18 li-bullet-0"><span class="c8 c9">At each iteration, the learning rate (</span><span class="c7 c9">&alpha;</span><span class="c0 c9">) is recalculated, and as the number of iterations increases, the learning rate decreases.</span></li></ul><p class="c6 c31 c9"><span class="c0 c9"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 422.10px; height: 281.40px;"><img alt="" src="images/image14.gif" style="width: 422.10px; height: 281.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c9"><span class="c0 c9"></span></p><p class="c2 c75"><span class="c1">Adadelta</span></p><p class="c6 c9 c75"><span class="c1"></span></p><ul class="c40 lst-kix_zbes9rxb87oe-0 start"><li class="c2 c18 li-bullet-0"><span class="c7 c9">Adadelta</span><span class="c0 c9">&nbsp;is an extension of the Adagrad optimizer, addressing its issue of aggressively reducing the learning rate.</span></li><li class="c2 c18 li-bullet-0"><span class="c0 c9">Instead of using past squared gradients, Adadelta uses a weighted average of all past squared gradients, which prevents the learning rate from becoming infinitesimally small.</span></li><li class="c2 c18 li-bullet-0"><span class="c0 c9">The weight update formula is similar to Adagrad, but the learning rate at each iteration is recalculated using the weighted average.</span></li><li class="c2 c18 li-bullet-0"><span class="c0 c9">A restricting term (&gamma; = 0.95) is used to avoid the Vanishing Gradient problem and helps stabilize learning.</span></li></ul><p class="c2 c76"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 373.59px; height: 301.93px;"><img alt="" src="images/image36.gif" style="width: 373.59px; height: 301.93px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c37"><span class="c1">RMSprop</span></p><ul class="c40 lst-kix_6rbjdsr9r5fa-0 start"><li class="c2 c22 li-bullet-0"><span class="c0 c9">RMSprop and Adadelta were developed to solve Adagrad&#39;s issue of diminishing learning rates.</span></li><li class="c2 c22 li-bullet-0"><span class="c0 c9">Both use an Exponential Weighted Average to calculate the learning rate at each iteration.</span></li><li class="c2 c22 li-bullet-0"><span class="c0 c9">RMSprop, proposed by Geoffrey Hinton, adapts the learning rate by dividing it by the exponentially weighted average of squared gradients.</span></li><li class="c2 c22 li-bullet-0"><span class="c0 c9">It is commonly recommended to set &gamma; (gamma) to 0.95, as it has shown good results in most cases.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 313.50px; height: 237.72px;"><img alt="" src="images/image6.gif" style="width: 492.76px; height: 314.86px; margin-left: -110.55px; margin-top: -36.21px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><p class="c6 c9 c25"><span class="c0 c9"></span></p><p class="c6 c9"><span class="c1"></span></p><p class="c6 c9"><span class="c5"></span></p><p class="c6 c9"><span class="c5"></span></p><p class="c6 c9"><span class="c5"></span></p><p class="c2"><span class="c1">Adam</span></p><ul class="c40 lst-kix_m0qsfktae37s-0 start"><li class="c2 c18 li-bullet-0"><span class="c0 c9">Adam (Adaptive Moment Estimation) is an optimization algorithm that computes adaptive learning rates for each parameter at every iteration.</span></li><li class="c2 c18 li-bullet-0"><span class="c0 c9">It combines Gradient Descent with Momentum and RMSprop to determine parameter updates.</span></li><li class="c2 c18 li-bullet-0"><span class="c0 c9">Adam is especially effective for non-convex optimization problems, making it one of the most widely used optimizers.</span></li></ul><p class="c2"><span class="c0 c9">It has several advantages, such as:</span></p><ul class="c40 lst-kix_tp8dcj8chtv6-0 start"><li class="c17 c23 c9 li-bullet-0"><span class="c0 c9">Low memory requirements.</span></li><li class="c17 c23 c9 li-bullet-0"><span class="c0 c9">Suitable for non-stationary objectives.</span></li><li class="c17 c23 c9 li-bullet-0"><span class="c0 c9">Works well with large data and parameters.</span></li><li class="c17 c23 c9 li-bullet-0"><span class="c0 c9">Efficient computation.</span></li></ul><p class="c2"><span class="c0 c9">Adam uses adaptive learning rates and stores an exponentially weighted average of the past squared gradients to update the weights.</span></p><p class="c6 c9"><span class="c0 c9"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 433.50px; height: 255.82px;"><img alt="" src="images/image22.png" style="width: 433.50px; height: 255.82px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c9"><span class="c1"></span></p><p class="c6 c9"><span class="c1"></span></p><hr><p class="c6 c9"><span class="c13 c9 c85"></span></p><p class="c2"><span class="c13 c9 c81">The </span><span class="c13 c81 c9 c89">Encoder</span></p><p class="c2"><span class="c8 c9">A structured dataset typically includes a mix of numerical and categorical variables. Machine learning algorithms can only process numerical data, not text. This is where categorical encoding comes into play.In </span><span class="c7 c9">scikit-learn</span><span class="c0 c9">, encoding refers to the process of converting categorical variables into a format that can be used by machine learning algorithms, typically by transformingcategorical data into numerical values. There are different types of encoders in scikit-learn to handle categorical data</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 486.50px; height: 221.86px;"><img alt="" src="images/image23.png" style="width: 486.50px; height: 221.86px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c13 c9 c86">Different types of Encoder in Sci-Kit Learn</span></p><p class="c27 c9 c19"><span class="c41 c7 c9"></span></p><ul class="c40 lst-kix_ej4nnpt3di2x-1 start"><li class="c61 c51 c9 c59 li-bullet-0"><span class="c41 c7 c9">One-Hot Encoding: Great for many categories, creates new binary features (1 for the category, 0 for others).</span></li><li class="c51 c9 c59 c61 li-bullet-0"><span class="c7 c9 c41">Label Encoding: Simple, assigns a number to each category, but assumes order matters (which might not be true).</span></li><li class="c27 c51 c9 c59 li-bullet-0"><span class="c41 c7 c9">Ordinal Encoding: Similar to label encoding, but only use it if categories have a natural order (like low, medium, high).</span></li></ul><p class="c9 c27"><span class="c24 c13 c9">Label Encoding</span></p><p class="c4 c9"><span class="c0 c9">Label Encoding is a common technique for converting categorical variables into numerical values. Each unique category value is assigned a unique integer based on alphabetical or numerical ordering.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 285.00px; height: 231.69px;"><img alt="" src="images/image19.png" style="width: 308.00px; height: 248.68px; margin-left: -10.00px; margin-top: -6.31px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c9 c19"><span class="c0 c9"></span></p><h3 class="c4 c9" id="h.i7o9kf8jzb8l"><span class="c9 c47 c83">Implementing Label Encoding </span></h3><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 326.15px; height: 148.73px;"><img alt="" src="images/image38.png" style="width: 326.15px; height: 148.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c17 c9"><span class="c7 c9">Challenges with Label Encoding:<br></span><span class="c0 c9">Label encoding can introduce an arbitrary order in categorical data, which may mislead machine learning models. For example, encoding countries (France, Germany, Spain) assigns integers (e.g., 0, 1, 2), creating an ordinal relationship (France &lt; Germany &lt; Spain), even though no inherent order exists. This can cause the model to incorrectly interpret categories as having a meaningful sequence.</span></p><p class="c17 c9"><span class="c8 c9">Understanding the differences between One-Hot Encoding and Label Encoding and using them correctly in tools like Pandas and Scikit-learn ensures efficient and accurate conversion of c</span><span class="c9 c64 c66">ategorical data, while avoiding misinterpretations.</span></p><p class="c6 c9"><span class="c41 c9 c64 c72"></span></p><p class="c2"><span class="c24 c13 c9">One-Hot Encoding (OHE)</span></p><p class="c2"><span class="c0 c9">One-Hot Encoding is another popular technique for treating categorical variables. It simply creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature. One-Hot Encoding is the process of creating dummy variables.</span></p><h3 class="c9 c56" id="h.xylofsf24byf"><span class="c42 c9 c47 c84">Implementing One-Hot Encoding using Scikit-Learn</span></h3><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 236.00px;"><img alt="" src="images/image31.png" style="width: 624.00px; height: 236.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c9"><span class="c0 c9"></span></p><p class="c2"><span class="c0 c9">As you can see, three new features are added because the Country column contains three unique values &ndash; France, Spain, and Germany. This method avoids the problem of ranking inherent in one hot encoding and label encoding, as each category is represented by a separate binary vector.</span></p><p class="c2"><span class="c24 c13 c9">OrdinalEncoder</span></p><p class="c17 c9"><span class="c7 c9">Ordinal Encoding</span><span class="c8 c9">&nbsp;is similar to </span><span class="c7 c9">Label Encoding</span><span class="c8 c9">&nbsp;but is specifically designed for </span><span class="c7 c9">ordinal data</span><span class="c0 c9">, where the categories have a meaningful order. It preserves the order of categories, making it ideal for data like grades (A, B, C) or levels (Low, Medium, High), where the sequence matters. This encoding helps machine learning algorithms, which function best with numerical data, process ordered categorical variables efficiently.</span></p><p class="c17 c9"><span class="c7 c9">Ordinal encoding</span><span class="c8 c9">&nbsp;converts categorical data into numeric values by assigning a unique integer to each category. It is ideal for variables with inherent order, like </span><span class="c7 c9">size (small &lt; medium &lt; large)</span><span class="c8 c9">. However, it is also applied to unordered categories since models like </span><span class="c7 c9">decision trees</span><span class="c8 c9">&nbsp;can still learn from arbitrary assignments. A key advantage over </span><span class="c7 c9">one-hot encoding</span><span class="c0 c9">&nbsp;is that it keeps the feature space compact, avoiding unnecessary dimensionality expansion.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 369.63px; height: 183.25px;"><img alt="" src="images/image35.png" style="width: 369.63px; height: 183.25px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c17 c9 c19"><span class="c0 c9"></span></p><p class="c17 c9 c19"><span class="c0 c9"></span></p><p class="c6 c9"><span class="c0 c9"></span></p><p class="c6 c9"><span class="c0 c9"></span></p><p class="c17 c9 c19"><span class="c24 c13 c9"></span></p><p class="c17 c9"><span class="c24 c13 c9">CategoricalEncoder</span></p><p class="c17 c9"><span class="c8 c9">The </span><span class="c7 c9">CategoricalEncoder</span><span class="c8 c9">&nbsp;was a class in earlier versions of </span><span class="c7 c9">scikit-learn</span><span class="c8 c9">&nbsp;used for encoding categorical data into numerical representations. It supported both </span><span class="c7 c9">one-hot encoding</span><span class="c8 c9">&nbsp;and </span><span class="c7 c9">ordinal encoding</span><span class="c0 c9">&nbsp;for categorical variables, allowing users to transform categorical features into a format that machine learning algorithms could process.</span></p><p class="c17 c9"><span class="c8 c9">However, </span><span class="c7 c9">CategoricalEncoder</span><span class="c8 c9">&nbsp;has been </span><span class="c7 c9">deprecated</span><span class="c8 c9">&nbsp;in </span><span class="c7 c9">scikit-learn</span><span class="c8 c9">&nbsp;due to its overlap with other more specific and effective encoders like </span><span class="c7 c9">OneHotEncoder</span><span class="c8 c9">&nbsp;and </span><span class="c7 c9">OrdinalEncoder</span><span class="c0 c9">, which provide better handling and functionality for encoding categorical features.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 575.50px; height: 273.60px;"><img alt="" src="images/image3.png" style="width: 575.50px; height: 273.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><div><p class="c6"><span class="c43"></span></p></div></body></html>